{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0a61948",
      "metadata": {
        "id": "f0a61948"
      },
      "source": [
        "# ğŸ”¹ Assignment 1: Seq2Seq Model for Sentence Unshuffling\n",
        "In this assignment, you'll implement a basic **Sequence-to-Sequence (Seq2Seq)** neural network using PyTorch.\n",
        "\n",
        "### ğŸ¯ **Your goal:**\n",
        "Given a sentence whose words have been shuffled randomly, your model must reconstruct the original sentence.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "| Input (shuffled) | Output (original) |\n",
        "|-----------------|------------------|\n",
        "| `mat the on sat cat The` | `The cat sat on the mat` |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37546a90",
      "metadata": {
        "id": "37546a90"
      },
      "source": [
        "## ğŸ”¹ Why this task?\n",
        "This simple yet non-trivial task demonstrates how language models learn word-order and syntactic structures.\n",
        "From a psycholinguistic viewpoint, sentence reconstruction taps into:\n",
        "\n",
        "- **Working memory**: The model must hold multiple words and reorder them meaningfully.\n",
        "- **Syntax and semantics**: Reordering depends on syntactic constraints and semantic coherence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FmlLBejDCw6X",
      "metadata": {
        "id": "FmlLBejDCw6X"
      },
      "source": [
        "## ğŸ”¹ Quick Summary of PyTorch Workflow\n",
        "\n",
        "The general workflow when working with on deep learning with PyTorch usually involves these steps:\n",
        "\n",
        "1. **Prepare your data**:\n",
        "    - Define your dataset by subclassing `torch.utils.data.Dataset`.\n",
        "    - Use a `DataLoader` to iterate efficiently over the dataset in batches.\n",
        "    - Tokenization - select tokenization method and tokenize your data.\n",
        "\n",
        "2. **Define your model**:\n",
        "    - Create a model class by subclassing `nn.Module`.\n",
        "    - Define model layers in `__init__`.\n",
        "    - Define how data flows through the layers in the `forward()` method.\n",
        "\n",
        "\n",
        "3. **Training**:\n",
        "    - Select an appropriate loss function (`nn.CrossEntropyLoss`, `nn.MSELoss`, etc.).\n",
        "    - Choose an optimizer (`optim.Adam`, `optim.SGD`, etc.).\n",
        "    - **Training Loop** -- For each batch:\n",
        "        1. Pass input data through your model to produce predictions (logits).\n",
        "        2. Compute the loss w.r.t. the gold label (target).\n",
        "        3. Perform backpropagation (`loss.backward()`) to calculate gradients of all model parameters.\n",
        "        4. Update the model parameters with your optimizer (`optimizer.step()`).\n",
        "        5. Reset gradients (`optimizer.zero_grad()`).\n",
        "\n",
        "This structured workflow helps streamline model development and makes training neural networks clear and efficient.\n",
        "\n",
        "This notebook will walk you through this process - you need to learn it and the complete the missing code segment marked with a `#TODO` comment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IMXO6OkGIU1j",
      "metadata": {
        "id": "IMXO6OkGIU1j"
      },
      "source": [
        "## ğŸ”¹ Step 0: One-time Preparations\n",
        "\n",
        "### Step 0.1: Install Python Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "hKyvKyjaIgWA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKyvKyjaIgWA",
        "outputId": "4658ab02-b8e6-4579-ee66-54104fe24a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "%pip install torch pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "SWIWKlTF1QFM",
        "outputId": "dc791911-60b6-4671-bae8-98899b9430a2"
      },
      "id": "SWIWKlTF1QFM",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2ed61070-de26-4618-84e5-6044b2ea2353\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2ed61070-de26-4618-84e5-6044b2ea2353\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_no_target.csv to test_no_target.csv\n",
            "Saving train.csv to train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7573a39",
      "metadata": {
        "id": "c7573a39"
      },
      "source": [
        "### Step 0.2: Download the Dataset\n",
        "Download the provided dataset file (`train.csv`) from the following link:\n",
        "\n",
        "[train.csv](https://drive.google.com/file/d/1eHBj_mdKjPfj_NuXy0zCG5IkQMJLGpPM/view?usp=sharing)\n",
        "\n",
        "Then upload the file to your Colab notebook or Jupyter environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d22c22e",
      "metadata": {
        "id": "8d22c22e"
      },
      "source": [
        "## ğŸ”¹ Step 1: Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a70075c2",
      "metadata": {
        "id": "a70075c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c813e4ae-db65-4c36-b6de-cfe731cd526d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                     input_sentence  \\\n",
              "0           0           to think need we about. That's something   \n",
              "1           1          is mountains. the up moon coming over The   \n",
              "2           2  committee. the The through Congressmen bill ra...   \n",
              "3           3            careful late I'll to never again. be be   \n",
              "4           4           please.\" gifts, \"No The said, invitation   \n",
              "\n",
              "                                     target_sentence  \n",
              "0           That's something we need to think about.  \n",
              "1          The moon is coming up over the mountains.  \n",
              "2  The Congressmen rammed the bill through commit...  \n",
              "3            I'll be careful never to be late again.  \n",
              "4           The invitation said, \"No gifts, please.\"  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b1c3e74-e0e5-4ec5-b9f1-4d1494a8792e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>input_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>to think need we about. That's something</td>\n",
              "      <td>That's something we need to think about.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>is mountains. the up moon coming over The</td>\n",
              "      <td>The moon is coming up over the mountains.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>committee. the The through Congressmen bill ra...</td>\n",
              "      <td>The Congressmen rammed the bill through commit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>careful late I'll to never again. be be</td>\n",
              "      <td>I'll be careful never to be late again.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>please.\" gifts, \"No The said, invitation</td>\n",
              "      <td>The invitation said, \"No gifts, please.\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b1c3e74-e0e5-4ec5-b9f1-4d1494a8792e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4b1c3e74-e0e5-4ec5-b9f1-4d1494a8792e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4b1c3e74-e0e5-4ec5-b9f1-4d1494a8792e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f11ef6a6-e393-4164-a7a5-4fffa74ae3af\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f11ef6a6-e393-4164-a7a5-4fffa74ae3af')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f11ef6a6-e393-4164-a7a5-4fffa74ae3af button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 35000,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10103,\n        \"min\": 0,\n        \"max\": 34999,\n        \"num_unique_values\": 35000,\n        \"samples\": [\n          17813,\n          6857,\n          7672\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34984,\n        \"samples\": [\n          \"in bridges There many this city. are\",\n          \"to at want resign present. my job I don't\",\n          \"My missing always was buses. father\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32293,\n        \"samples\": [\n          \"I asked him to slowly read off the numbers on the meter.\",\n          \"Bill Clinton denied the accusation.\",\n          \"It is not you but her that he wants to marry.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_seed = 123\n",
        "random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c21b2edd",
      "metadata": {
        "id": "c21b2edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e83ff9-872a-40fd-9bc5-1ea13674a12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 31500\n",
            "Development examples: 3500\n"
          ]
        }
      ],
      "source": [
        "train_df, dev_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "print(f\"Training examples: {len(train_df)}\")\n",
        "print(f\"Development examples: {len(dev_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JRGMPu8eKVnI",
      "metadata": {
        "id": "JRGMPu8eKVnI"
      },
      "source": [
        "#### Pedagogical Note: **Why do we split train data into training and development (dev) sets?**\n",
        "\n",
        "When developing machine learning models, we want to ensure our model not only performs well on the data it has seen during training but also generalizes effectively to **new, unseen data**.\n",
        "\n",
        "- **Training set**:  \n",
        "  Used by the model to learn patterns. This is the data your model sees repeatedly during the training process.\n",
        "\n",
        "- **Development (dev) set** *(also known as validation set)*:  \n",
        "  Used to evaluate the model's performance on unseen examples during training. By checking the model periodically against the dev set, we can identify and prevent **overfitting**â€”when the model performs excellently on training data but poorly on new examples.\n",
        "\n",
        "Thus, by splitting the data, we ensure our model truly learns generalizable patterns rather than memorizing the specific examples it trained on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9df6d543",
      "metadata": {
        "id": "9df6d543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a3f91d-f15e-4a8e-8488-cd6d6f765d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 8531\n"
          ]
        }
      ],
      "source": [
        "# Handle Vocabulary and Tokenization\n",
        "from collections import Counter  # for count the number of words easy\n",
        "\n",
        "def tokenize(sentence):\n",
        "    return sentence.lower().split() # convert to lowercase letters and then split by spaces\n",
        "\n",
        "counter = Counter()\n",
        "for sentence in train_df['input_sentence']:\n",
        "    counter.update(tokenize(sentence))\n",
        "\n",
        "# Special Tokens\n",
        "PAD = '<PAD>' # to fill in when sentence is too short\n",
        "SOS = '<SOS>' # start of sentence\n",
        "EOS = '<EOS>' # end of sentence\n",
        "UNK = '<UNK>'   # to handle out-of-vocabulary words - unknown\n",
        "\n",
        "words = [PAD, SOS, EOS, UNK] + [w for w, c in counter.items() if c >= 2] # save only words that shows at least twice\n",
        "# we maintain mapping between words (vocabulary entries) and their ids\n",
        "word2idx = {w: i for i, w in enumerate(words)} # convert from word to number\n",
        "idx2word = {i: w for w, i in word2idx.items()} # convert from number to word\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "\n",
        "## this next function get a sentence and convert it to list of numbers in the same length for all\n",
        "## this is done so that the sentence will be ready to be fed into the model\n",
        "\n",
        "def encode_sentence(sentence: str, word2idx, max_len):\n",
        "    # Replace sentence string with a fixed-length list of ints (token_ids with padding)\n",
        "    tokens = tokenize(sentence) # divide the sentence into words-tokens\n",
        "    token_ids = [word2idx.get(w, word2idx[UNK]) for w in tokens]\n",
        "    token_ids = token_ids[:max_len-1] # trim the list so that it is shorter one token  than max_len\n",
        "    token_ids.append(word2idx[EOS]) # add the EOS token to the end to mark sentence termination\n",
        "    padding = [word2idx[PAD]] * (max_len - len(token_ids)) # add PAD tokens if needed to make the sentence exactly max_len long\n",
        "    return token_ids + padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9a1b859c",
      "metadata": {
        "id": "9a1b859c"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class for sentence-to-sentence mapping\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, word2idx, max_len):\n",
        "        # Store input and target sentences as lists of strings\n",
        "        self.input_sentences = df['input_sentence'].tolist() # input: shuffled sentence\n",
        "        self.target_sentences = df['target_sentence'].tolist() # target: original sentence\n",
        "        self.word2idx = word2idx  # mapping from word to index\n",
        "        self.max_len = max_len    # maximum length of sentence (for padding)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return number of examples in the dataset\n",
        "        return len(self.input_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Encode both input and target sentences to fixed-length tensors of token IDs\n",
        "        src = encode_sentence(self.input_sentences[idx], self.word2idx, self.max_len)\n",
        "        trg = encode_sentence(self.target_sentences[idx], self.word2idx, self.max_len)\n",
        "        return torch.tensor(src), torch.tensor(trg)  # return as PyTorch tensors\n",
        "\n",
        "# Define batch size and maximum sentence length\n",
        "# this is the hyperparameters\n",
        "batch_size = 32\n",
        "max_len = 50  # All sequences will be padded/truncated to this length\n",
        "\n",
        "# Create training dataset and dataloader\n",
        "train_dataset = SentenceDataset(train_df, word2idx, max_len)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "# DataLoader loads batches from the dataset and optionally shuffles them\n",
        "\n",
        "# Create development (validation) dataset and dataloader\n",
        "dev_dataset = SentenceDataset(dev_df, word2idx, max_len)\n",
        "dev_loader = torch.utils.data.DataLoader(\n",
        "    dev_dataset, batch_size=batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7o3Mz8zr3foD",
      "metadata": {
        "id": "7o3Mz8zr3foD"
      },
      "source": [
        "## PyTorch Quick Reference\n",
        "\n",
        "Since this is your first encounter with PyTorch in the course, here's a short summary of essential PyTorch classes used in this notebook:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hKLOmTr0BHyk",
      "metadata": {
        "id": "hKLOmTr0BHyk"
      },
      "source": [
        "\n",
        "---\n",
        "### âœ… torch.utils.data.Dataset\n",
        "#### What is it?\n",
        "\n",
        "An abstract class representing your dataset. It lets you define exactly how to access and prepare each data point.\n",
        "\n",
        "#### How to use it?\n",
        "\n",
        "You subclass it and implement two methods:\n",
        "\n",
        "`__len__(self)`: returns the size of your dataset.\n",
        "\n",
        "`__getitem__(self, index)`: returns one data point (input-target pair).\n",
        "\n",
        "#### Why do we use it?\n",
        "It provides a clean way to structure your data and feed it systematically into your model.\n",
        "\n",
        "---\n",
        "### âœ… torch.utils.data.DataLoader\n",
        "#### What is it?\n",
        "A utility that takes a Dataset object and provides an iterator over it.\n",
        "\n",
        "#### How to use it?\n",
        "Specify batch size, shuffle options, and more:\n",
        "\n",
        "```python\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "```\n",
        "#### Why do we use it?\n",
        "It handles batching, shuffling, and efficient parallel data loading automaticallyâ€”making your training loop concise and efficient.\n",
        "\n",
        "---\n",
        "### âœ… torch.nn.Module\n",
        "#### What is it?\n",
        "The base class for all neural network models in PyTorch. Every model you build inherits from it.\n",
        "\n",
        "#### How to use it?\n",
        "You subclass it, define your layers in the constructor (`__init__`), and specify the forward pass in the `forward()` method:\n",
        "\n",
        "```python\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.linear = nn.Linear(10, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "```\n",
        "#### Why do we use it?\n",
        "It manages model parameters, handles the forward computation, and simplifies tasks such as moving models to GPU or tracking gradients automatically.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VlTEAM4y3FA0",
      "metadata": {
        "id": "VlTEAM4y3FA0"
      },
      "source": [
        "## ğŸ”¹ Step 2: Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DuXH8zUee6Ud",
      "metadata": {
        "id": "DuXH8zUee6Ud"
      },
      "source": [
        "### Building blocks - Enocder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6c990ab2",
      "metadata": {
        "id": "6c990ab2"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        # TODO: define layers used in the encoder, e.g. embedding and LSTM layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size) # convert every word into vector in size 64\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, dropout = 0.3) # send this vectors to LSTM, and we add dropout to prevent overfit\n",
        "\n",
        "        # the function forward is how the data pass through the model\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"\n",
        "        input: (seq_len, batch_size) - token indices for input sentence\n",
        "        hidden: (1, batch_size, hidden_size) - initial hidden state\n",
        "        Returns:\n",
        "            output: all hidden states for the input sequence\n",
        "            hidden: final hidden state\n",
        "        \"\"\"\n",
        "        # TODO: implement the forward pass here\n",
        "        embedded = self.embedding(input)  # (seq_len, batch, embedding_size)\n",
        "        output, hidden = self.lstm(embedded, hidden)  # pass the vectors into the LSTM\n",
        "        return output, hidden\n",
        "\n",
        "    #def init_hidden(self, batch_size):\n",
        "    #    return torch.zeros(1, batch_size, hidden_size)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "    # move both h_0 and c_0 to the correct device\n",
        "      return (\n",
        "        torch.zeros(1, batch_size, self.hidden_size).to(self.embedding.weight.device),\n",
        "        torch.zeros(1, batch_size, self.hidden_size).to(self.embedding.weight.device)\n",
        "    )\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        # TODO: define layers used in the decoder, e.g. embedding, LSTM and linear layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)  # words to vectors\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size)  # vectors from before to LSTM\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)  # layer for predict the next word\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"\n",
        "        input: (batch_size) - current token index at this decoding step\n",
        "        hidden: (1, batch_size, hidden_size) - current hidden state\n",
        "        Returns:\n",
        "            output: prediction for the next word (before softmax)\n",
        "            hidden: updated hidden state\n",
        "        \"\"\"\n",
        "        # TODO: Embed the input token, run one step of the RNN, generate output\n",
        "        embedded = self.embedding(input).unsqueeze(0)  # # add time dimension\n",
        "        output, hidden = self.lstm(embedded, hidden)  # run one step of LSTM\n",
        "        output = self.out(output.squeeze(0))  # predict next word\n",
        "        return output, hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "08f70b0d",
      "metadata": {
        "id": "08f70b0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e101936-cfaa-4872-a652-ded97dfec5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: torch.Size([50, 32, 128])\n",
            "Decoder output shape: torch.Size([32, 8531])\n"
          ]
        }
      ],
      "source": [
        "# Define Encoder & Decoder\n",
        "embedding_size = 64\n",
        "hidden_size = 128\n",
        "\n",
        "encoder = EncoderRNN(vocab_size, embedding_size, hidden_size)\n",
        "decoder = DecoderRNN(vocab_size, embedding_size, hidden_size)\n",
        "\n",
        "# Sanity Checks\n",
        "sample_input, sample_target = next(iter(train_loader))\n",
        "encoder_hidden = encoder.init_hidden(sample_input.size(0))\n",
        "encoder_output, encoder_hidden = encoder(sample_input.T, encoder_hidden)\n",
        "\n",
        "decoder_input = torch.tensor([word2idx[SOS]] * sample_input.size(0))\n",
        "decoder_hidden = encoder_hidden\n",
        "\n",
        "decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "print(\"Encoder output shape:\", encoder_output.shape)\n",
        "print(\"Decoder output shape:\", decoder_output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jQLknjFt-5bg",
      "metadata": {
        "id": "jQLknjFt-5bg"
      },
      "source": [
        "### Seq2Seq model that wraps Encoder and Decoder together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "JdusuF9w_Ea4",
      "metadata": {
        "id": "JdusuF9w_Ea4"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module): # encoder decoder in one piece\n",
        "    def __init__(self, encoder, decoder, sos_idx, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sos_idx = sos_idx\n",
        "        self.device = device  # needed for creating new tensors like hidden states\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        src: (batch_size, seq_len) - input sentence (shuffled)\n",
        "        trg: (batch_size, seq_len) - target sentence (original)\n",
        "        Returns:\n",
        "            outputs: tensor of shape (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, trg_len = trg.shape\n",
        "        vocab_size = self.decoder.out.out_features\n",
        "\n",
        "        # Create an empty tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, trg_len, vocab_size, device=self.device)\n",
        "\n",
        "        # Transpose inputs to match (seq_len, batch_size)\n",
        "        src = src.T\n",
        "        trg = trg.T\n",
        "\n",
        "        # Initialize hidden state on correct device\n",
        "        hidden = self.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Encode the input sentence\n",
        "        _, hidden = self.encoder(src, hidden)\n",
        "\n",
        "        # Start decoding with the <SOS> token\n",
        "        input = torch.full((batch_size,), self.sos_idx, device=self.device)\n",
        "\n",
        "        for t in range(trg_len):\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            outputs[:, t, :] = output\n",
        "            input = trg[t]  # teacher forcing: feed the true token at each step\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WAmJH-5feIkb",
      "metadata": {
        "id": "WAmJH-5feIkb"
      },
      "source": [
        "## ğŸ”¹ Step 3: Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0lUGBZ7pd0Rw",
      "metadata": {
        "id": "0lUGBZ7pd0Rw"
      },
      "outputs": [],
      "source": [
        "# define the training-loop function\n",
        "def train_model(model, train_loader, dev_loader, optimizer, criterion, device, num_epochs=5):\n",
        "    model.to(device)\n",
        "    train_losses = []\n",
        "    dev_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for src, trg in train_loader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, trg)\n",
        "\n",
        "            # output: (batch, seq_len, vocab_size)\n",
        "            # trg: (batch, seq_len)\n",
        "            output = output.view(-1, output.shape[-1])  # shape: (batch * seq_len, vocab_size)\n",
        "            trg = trg.view(-1)                          # shape: (batch * seq_len)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "        # Evaluate on dev set\n",
        "        model.eval()\n",
        "        dev_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for src, trg in dev_loader:\n",
        "                src, trg = src.to(device), trg.to(device)\n",
        "                output = model(src, trg)\n",
        "                output = output.view(-1, output.shape[-1])\n",
        "                trg = trg.view(-1)\n",
        "                loss = criterion(output, trg)\n",
        "                dev_loss += loss.item()\n",
        "\n",
        "        dev_losses.append(dev_loss / len(dev_loader))\n",
        "\n",
        "        # --- Clean and formatted output ---\n",
        "        bar = \"=\" * 60\n",
        "        print(f\"\\n{bar}\")\n",
        "        print(f\"{'EPOCH':<12}: {epoch + 1} / {num_epochs}\")\n",
        "        print(f\"{'Train Loss':<12}: {train_losses[-1]:.4f}\")\n",
        "        print(f\"{'Dev Loss':<12}: {dev_losses[-1]:.4f}\")\n",
        "        print(f\"{bar}\\n\")\n",
        "\n",
        "    return train_losses, dev_losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "AqiWK2_-fOWF",
      "metadata": {
        "id": "AqiWK2_-fOWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b9340b-ffb6-4bf6-87ba-9422c3ef52e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH       : 1 / 12\n",
            "Train Loss  : 5.4503\n",
            "Dev Loss    : 4.6707\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 2 / 12\n",
            "Train Loss  : 4.5246\n",
            "Dev Loss    : 4.1750\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 3 / 12\n",
            "Train Loss  : 4.0823\n",
            "Dev Loss    : 3.8565\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 4 / 12\n",
            "Train Loss  : 3.6732\n",
            "Dev Loss    : 3.5343\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 5 / 12\n",
            "Train Loss  : 3.3198\n",
            "Dev Loss    : 3.3058\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 6 / 12\n",
            "Train Loss  : 3.0100\n",
            "Dev Loss    : 3.0963\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 7 / 12\n",
            "Train Loss  : 2.7422\n",
            "Dev Loss    : 2.9496\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 8 / 12\n",
            "Train Loss  : 2.5119\n",
            "Dev Loss    : 2.8309\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 9 / 12\n",
            "Train Loss  : 2.3160\n",
            "Dev Loss    : 2.7399\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 10 / 12\n",
            "Train Loss  : 2.1469\n",
            "Dev Loss    : 2.6777\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 11 / 12\n",
            "Train Loss  : 1.9954\n",
            "Dev Loss    : 2.6186\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 12 / 12\n",
            "Train Loss  : 1.8606\n",
            "Dev Loss    : 2.5783\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instantiate Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Seq2Seq(encoder, decoder, word2idx[SOS], device)\n",
        "\n",
        "# Choose optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[PAD])\n",
        "\n",
        "# Run training\n",
        "train_losses, dev_losses = train_model(\n",
        "    model, train_loader, dev_loader,\n",
        "    optimizer, criterion, device,\n",
        "    num_epochs=12\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "k63dQnCoh3Zn",
      "metadata": {
        "id": "k63dQnCoh3Zn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "faae1054-de1a-4640-d69f-624ea07e598d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAgsNJREFUeJzt3XdcVfUfx/HXvXC57C1LEREHiIpbce+ZinuP0izT0sp+7XJktsxKc1dauWdWLhy498S9QZGpsmWf3x9XKMKBeuEyPs/H4zzkHs743C9XeXvO93y/KkVRFIQQQgghSgi1oQsQQgghhNAnCTdCCCGEKFEk3AghhBCiRJFwI4QQQogSRcKNEEIIIUoUCTdCCCGEKFEk3AghhBCiRJFwI4QQQogSRcKNEEIIIUoUCTei1Bo+fDgVKlR4pn0nTpyISqXSb0FFzI0bN1CpVCxatMjQpQiRLy1btqR69eqGLkMUARJuRJGjUqnytQQFBRm6VAEEBQXl+rlotVqcnZ1p2bIln3/+OdHR0QarTaVSMXbsWIOdv6Rp2bLlI/8+ent7G7o8IXIYG7oAIf7rt99+y/X6119/JTAwMM96Hx+f5zrPggULyMrKeqZ9P/roI957773nOn9J88Ybb1C/fn0yMzOJjo5m//79fPrpp3z77besXLmS1q1bG7pEoQflypVj2rRpedbb2NgYoBohHk7CjShyBg8enOv1wYMHCQwMzLP+v5KTkzE3N8/3eTQazTPVB2BsbIyxsfz1+bdmzZrRu3fvXOtOnTpF+/bt6dWrF+fOncPV1dVA1Yn8yMrKIi0tDVNT00duY2Nj88S/i0IYmtyWEsVS9r31Y8eO0bx5c8zNzfnggw8A+OOPP+jSpQtubm5otVq8vLyYMmUKmZmZuY7x3z432X1MvvnmG+bPn4+XlxdarZb69etz5MiRXPs+rM9N9i2Q9evXU716dbRaLb6+vmzevDlP/UFBQdSrVw9TU1O8vLyYN29evvvx7Nmzhz59+lC+fHm0Wi3u7u68+eab3L9/P8/7s7S0JCwsjICAACwtLSlTpgwTJkzI0xaxsbEMHz4cGxsbbG1tGTZsGLGxsU+s5Un8/Pz47rvviI2NZdasWbm+FxYWxksvvYSzs3NOW/38888534+MjMTY2JhJkyblOe7FixdRqVR5jvkskpKSePvtt3F3d0er1VK1alW++eYbFEXJtV1gYCBNmzbF1tYWS0tLqlatmvOZyzZz5kx8fX0xNzfHzs6OevXqsXTp0ifWEBUVxYgRI3B2dsbU1BQ/Pz8WL16c8/309HTs7e158cUX8+wbHx+PqakpEyZMyFmXmprKp59+SqVKlXI+I//73/9ITU3NtW/2Z3bJkiX4+vqi1Wof+nl9Wtmf5QsXLtC3b1+sra1xcHBg3LhxpKSk5No2IyODKVOm5Px9q1ChAh988EGeWgE2bdpEixYtsLKywtramvr16z+0fc+dO0erVq0wNzenbNmyfPXVV3m2edaflSge5L+eoti6c+cOnTp1on///gwePBhnZ2cAFi1ahKWlJW+99RaWlpbs2LGDTz75hPj4eL7++usnHnfp0qUkJCTwyiuvoFKp+Oqrr+jZsyfXrl174tWevXv3snbtWl577TWsrKz44Ycf6NWrF6GhoTg4OABw4sQJOnbsiKurK5MmTSIzM5PJkydTpkyZfL3vVatWkZyczOjRo3FwcODw4cPMnDmTW7dusWrVqlzbZmZm0qFDBxo2bMg333zDtm3bmD59Ol5eXowePRoARVHo3r07e/fu5dVXX8XHx4d169YxbNiwfNXzJL1792bEiBFs3bqVqVOnArrg0qhRo5xfrmXKlGHTpk2MGDGC+Ph4xo8fj7OzMy1atGDlypV8+umnuY65YsUKjIyM6NOnz3PVpigK3bp1Y+fOnYwYMYJatWqxZcsW3nnnHcLCwpgxYwYAZ8+e5YUXXqBmzZpMnjwZrVbLlStX2LdvX86xFixYwBtvvEHv3r1zfomfPn2aQ4cOMXDgwEfWcP/+fVq2bMmVK1cYO3Ysnp6erFq1iuHDhxMbG8u4cePQaDT06NGDtWvXMm/ePExMTHL2X79+PampqfTv3x/QXX3p1q0be/fuZdSoUfj4+BAcHMyMGTO4dOkS69evz3X+HTt2sHLlSsaOHYujo+MTO9lnZmYSExOTZ72ZmRkWFha51vXt25cKFSowbdo0Dh48yA8//MC9e/f49ddfc7YZOXIkixcvpnfv3rz99tscOnSIadOmcf78edatW5ez3aJFi3jppZfw9fXl/fffx9bWlhMnTrB58+Zc7Xvv3j06duxIz5496du3L6tXr+bdd9+lRo0adOrU6bl+VqIYUYQo4saMGaP896PaokULBVDmzp2bZ/vk5OQ861555RXF3NxcSUlJyVk3bNgwxcPDI+f19evXFUBxcHBQ7t69m7P+jz/+UADlzz//zFn36aef5qkJUExMTJQrV67krDt16pQCKDNnzsxZ17VrV8Xc3FwJCwvLWXf58mXF2Ng4zzEf5mHvb9q0aYpKpVJCQkJyvT9AmTx5cq5ta9eurdStWzfn9fr16xVA+eqrr3LWZWRkKM2aNVMA5ZdffnlsPTt37lQAZdWqVY/cxs/PT7Gzs8t5PWLECMXV1VWJiYnJtV3//v0VGxubnPc4b948BVCCg4NzbVetWjWldevWj61LUXQ/kzFjxjzy+9nv/bPPPsu1vnfv3opKpcr5Wc6YMUMBlOjo6Eceq3v37oqvr+8Ta/qv7777TgGU33//PWddWlqa4u/vr1haWirx8fGKoijKli1b8nwOFUVROnfurFSsWDHn9W+//aao1Wplz549ubabO3euAij79u3LWQcoarVaOXv2bL5qzf5797DllVdeydku++9Ht27dcu3/2muvKYBy6tQpRVEU5eTJkwqgjBw5Mtd2EyZMUABlx44diqIoSmxsrGJlZaU0bNhQuX//fq5ts7Ky8tT366+/5qxLTU1VXFxclF69euWse9aflSg+5LaUKLa0Wu1DL9ObmZnlfJ2QkEBMTAzNmjUjOTmZCxcuPPG4/fr1w87OLud1s2bNALh27doT923bti1eXl45r2vWrIm1tXXOvpmZmWzbto2AgADc3NxytqtUqVLO/yqf5N/vLykpiZiYGBo3boyiKJw4cSLP9q+++mqu182aNcv1XjZu3IixsXHOlRwAIyMjXn/99XzVkx+WlpYkJCQAuqsla9asoWvXriiKQkxMTM7SoUMH4uLiOH78OAA9e/bE2NiYFStW5BzrzJkznDt3jn79+j13XRs3bsTIyIg33ngj1/q3334bRVHYtGkTALa2toDuluejOqHb2tpy69atPLcw81ODi4sLAwYMyFmn0Wh44403SExMZNeuXQC0bt0aR0fHXG1x7949AgMDc7XFqlWr8PHxwdvbO1fbZnfo3rlzZ67zt2jRgmrVquW73goVKhAYGJhnGT9+fJ5tx4wZk+t19mdq48aNuf586623cm339ttvA/D3338DuluCCQkJvPfee3n6A/33Vq6lpWWuPkEmJiY0aNAg12f+WX9WoviQcCOKrbJly+a6PJ/t7Nmz9OjRAxsbG6ytrSlTpkzOP3ZxcXFPPG758uVzvc4OOvfu3XvqfbP3z943KiqK+/fvU6lSpTzbPWzdw4SGhjJ8+HDs7e1z+tG0aNECyPv+TE1N89zu+nc9ACEhIbi6umJpaZlru6pVq+arnvxITEzEysoKgOjoaGJjY5k/fz5lypTJtWSH1aioKAAcHR1p06YNK1euzDnWihUrMDY2pmfPns9dV0hICG5ubjm1Zct+Ei8kJATQBd4mTZowcuRInJ2d6d+/PytXrswVdN59910sLS1p0KABlStXZsyYMbluWz2uhsqVK6NW5/7n+L81GBsb06tXL/7444+c/ihr164lPT09V7i5fPkyZ8+ezdO2VapUAf5p22yenp5Pbqh/sbCwoG3btnmWhz0KXrly5Vyvvby8UKvV3LhxI+e9qdXqPJ99FxcXbG1tc9771atXAfI1hk25cuXyBJ7/fuaf9Wclig/pcyOKrX9fwcgWGxtLixYtsLa2ZvLkyXh5eWFqasrx48d599138/Xot5GR0UPXK//pYKrvffMjMzOTdu3acffuXd599128vb2xsLAgLCyM4cOH53l/j6qnMKWnp3Pp0qWcX0zZNQ4ePPiR/Xpq1qyZ83X//v158cUXOXnyJLVq1WLlypW0adMGR0fHgi/+ATMzM3bv3s3OnTv5+++/2bx5MytWrKB169Zs3boVIyMjfHx8uHjxIn/99RebN29mzZo1zJ49m08++eShnaKfRf/+/Zk3bx6bNm0iICCAlStX4u3tjZ+fX842WVlZ1KhRg2+//fahx3B3d8/z3grLozrM63NAzPz8HSyMn5UwLAk3okQJCgrizp07rF27lubNm+esv379ugGr+oeTkxOmpqZcuXIlz/cetu6/goODuXTpEosXL2bo0KE56wMDA5+5Jg8PD7Zv305iYmKuqzcXL1585mP+2+rVq7l//z4dOnQAoEyZMlhZWZGZmUnbtm2fuH9AQACvvPJKzu2YS5cu8f777+ulNg8PD7Zt20ZCQkKuqzfZty89PDxy1qnVatq0aUObNm349ttv+fzzz/nwww/ZuXNnzvuwsLCgX79+9OvXj7S0NHr27MnUqVN5//33H/l4tYeHB6dPnyYrKyvX1ZuH1dC8eXNcXV1ZsWIFTZs2ZceOHXz44Ye5jufl5cWpU6do06aNwUfRvnz5cq4rQ1euXCErKyun07KHhwdZWVlcvnw517hVkZGRxMbG5rz37Fu9Z86cyfcVzid5lp+VKD7ktpQoUbL/1/bv/6WlpaUxe/ZsQ5WUi5GREW3btmX9+vXcvn07Z/2VK1dy+nc8aX/I/f4UReH7779/5po6d+5MRkYGc+bMyVmXmZnJzJkzn/mY2U6dOsX48eOxs7PL6X9hZGREr169WLNmDWfOnMmzz39HNLa1taVDhw6sXLmS5cuXY2JiQkBAwHPXBrr3npmZmeeR8hkzZqBSqXL6Qd29ezfPvrVq1QLIuUV0586dXN83MTGhWrVqKIpCenr6Y2uIiIjI1ZcmIyODmTNnYmlpmXPLEXQBq3fv3vz555/89ttvZGRk5Ol71LdvX8LCwliwYEGec92/f5+kpKRH1qJvP/74Y67X2Z+p7Hbt3LkzAN99912u7bKvOnXp0gWA9u3bY2VlxbRp0/I8Sv4sV0Wf9Wclig+5ciNKlMaNG2NnZ8ewYcN44403UKlU/Pbbb3q7LaQPEydOZOvWrTRp0oTRo0fn/HKtXr06J0+efOy+3t7eeHl5MWHCBMLCwrC2tmbNmjX56g/0KF27dqVJkya899573Lhxg2rVqrF27dp89U/6tz179pCSkkJmZiZ37txh3759bNiwARsbG9atW4eLi0vOtl988QU7d+6kYcOGvPzyy1SrVo27d+9y/Phxtm3blidM9OvXj8GDBzN79mw6dOiQ08E3P44ePcpnn32WZ33Lli3p2rUrrVq14sMPP+TGjRv4+fmxdetW/vjjD8aPH59zxWDy5Mns3r2bLl264OHhQVRUFLNnz6ZcuXI0bdoU0P0CdnFxoUmTJjg7O3P+/HlmzZpFly5d8vTp+bdRo0Yxb948hg8fzrFjx6hQoQKrV69m3759fPfdd3n27devHzNnzuTTTz+lRo0aeUbqHjJkCCtXruTVV19l586dNGnShMzMTC5cuMDKlSvZsmUL9erVy3f7/VdcXBy///77Q7/338H9rl+/Trdu3ejYsSMHDhzg999/Z+DAgTm30fz8/Bg2bBjz58/PuaV8+PBhFi9eTEBAAK1atQLA2tqaGTNmMHLkSOrXr8/AgQOxs7Pj1KlTJCcn5xoTKD+e9WclihFDPKIlxNN41KPgj3qUc9++fUqjRo0UMzMzxc3NTfnf//6X8xjtzp07c7Z71KPgX3/9dZ5jAsqnn36a8/pRj4I/7LFjDw8PZdiwYbnWbd++Xaldu7ZiYmKieHl5KQsXLlTefvttxdTU9BGt8I9z584pbdu2VSwtLRVHR0fl5Zdfznnk/N+PbQ8bNkyxsLDIs//Dar9z544yZMgQxdraWrGxsVGGDBminDhx4qkeBc9eNBqNUqZMGaV58+bK1KlTlaioqIfuFxkZqYwZM0Zxd3dXNBqN4uLiorRp00aZP39+nm3j4+MVMzOzPI9MPwkPeWQ5e5kyZYqiKIqSkJCgvPnmm4qbm5ui0WiUypUrK19//XWuR4y3b9+udO/eXXFzc1NMTEwUNzc3ZcCAAcqlS5dytpk3b57SvHlzxcHBQdFqtYqXl5fyzjvvKHFxcU+sMzIyUnnxxRcVR0dHxcTERKlRo8Yj2z0rK0txd3d/6CPs2dLS0pQvv/xS8fX1VbRarWJnZ6fUrVtXmTRpUq56HvWZfZTHPQr+789U9mfs3LlzSu/evRUrKyvFzs5OGTt2bJ5HudPT05VJkyYpnp6eikajUdzd3ZX3338/17AN2TZs2KA0btxYMTMzU6ytrZUGDRooy5Yty1Xfw/5d+O/f9ef5WYniQaUoRei/tEKUYgEBAZw9e5bLly8buhQhnsvEiROZNGkS0dHRhdrxW4hs0udGCAP471QJly9fZuPGjbRs2dIwBQkhRAkifW6EMICKFSsyfPhwKlasSEhICHPmzMHExIT//e9/hi5NCCGKPQk3QhhAx44dWbZsGREREWi1Wvz9/fn888/zDHomhBDi6UmfGyGEEEKUKNLnRgghhBAlioQbIYQQQpQopa7PTVZWFrdv38bKysrgQ5MLIYQQIn8URSEhIQE3N7c8E83+V6kLN7dv384zcZwQQgghioebN29Srly5x25T6sJN9tDaN2/exNraWq/HTk9PZ+vWrbRv3x6NRqPXY5cm0o76Ie2oH9KO+iHtqB+luR3j4+Nxd3fP1xQZpS7cZN+Ksra2LpBwY25ujrW1dan70OmTtKN+SDvqh7Sjfkg76oe0I/nqUiIdioUQQghRoki4EUIIIUSJIuFGCCGEECVKqetzI4QQomTJzMwkPT3d0GUUivT0dIyNjUlJSSEzM9PQ5eiVRqPByMhIL8eScCOEEKJYUhSFiIgIYmNjDV1KoVEUBRcXF27evFkix2qztbXFxcXlud+bQcPNxIkTmTRpUq51VatW5cKFCw/dftGiRbz44ou51mm1WlJSUgqsRiGEEEVTdrBxcnLC3Ny8RP6y/6+srCwSExOxtLR84kB2xYmiKCQnJxMVFQWAq6vrcx3P4FdufH192bZtW85rY+PHl2Rtbc3FixdzXpeGD7MQQojcMjMzc4KNg4ODocspNFlZWaSlpWFqalqiwg2AmZkZAFFRUTg5OT3XLSqDhxtjY2NcXFzyvb1KpXqq7YUQQpQ82X1szM3NDVyJ0Kfsn2d6enrxDjeXL1/Gzc0NU1NT/P39mTZtGuXLl3/k9omJiXh4eJCVlUWdOnX4/PPP8fX1feT2qamppKam5ryOj48HdA2n7w5o2ccrLR3bCoq0o35IO+qHtKN+6Lsd09PTURQFRVHIysrSyzGLA0VRcv4sie87+2f6sHDzNJ8dlZLdUgawadMmEhMTqVq1KuHh4UyaNImwsDDOnDnz0OGVDxw4wOXLl6lZsyZxcXF888037N69m7Nnzz5ynomH9esBWLp0qSR+IYQoprKv+ru7u2NiYmLocoSepKWlcfPmTSIiIsjIyMj1veTkZAYOHEhcXNwTZxgwaLj5r9jYWDw8PPj2228ZMWLEE7dPT0/Hx8eHAQMGMGXKlIdu87ArN+7u7sTExBTI9AuBgYG0a9eu1A6LrQ/Sjvoh7agf0o76oe92TElJ4ebNm1SoUAFTU1M9VFg8ZM+MbWVllavPacWKFRk3bhzjxo0zYHXPLyUlhRs3buDu7p7n5xofH4+jo2O+wo3Bb0v9m62tLVWqVOHKlSv52l6j0VC7du3Hbq/VatFqtQ/dt6D+oSrIY5cm0o76Ie2oH9KO+qGvdszMzESlUqFWq4tVx9onPQTz6aefMnHixEd+P/tWVPZ7z3bkyBEsLCyeqy1atmxJrVq1+O677575GM9LrVajUqke+jl5ms9NkfpEJCYmcvXq1Xw/ApaZmUlwcPBzPzKmL9djkoiRp9KFEEI8Qnh4eM7y3XffYW1tnWvdhAkTcrZVFCXPrZlHKVOmjHS1+BeDhpsJEyawa9cubty4wf79++nRowdGRkYMGDAAgKFDh/L+++/nbD958mS2bt3KtWvXOH78OIMHDyYkJISRI0ca6i3k+HnvdTr8sI+/Q4tUXhRCCFGEuLi45Cw2NjY5TwC7uLhw4cIFrKys2LRpE3Xr1kWr1bJ3716uXr1K9+7dcXZ2xtramtatW+caQgWgQoUKua64qFQqFi5cSI8ePTA3N6dy5cps2LDhuWpfs2YNvr6+aLVaKlSowPTp03N9f/bs2VSuXBlTU1OcnZ3p3bt3zvdWr15NjRo1MDMzw8HBgbZt25KUlPRc9TyOQW9L3bp1iwEDBnDnzh3KlClD06ZNOXjwIGXKlAEgNDQ01yW2e/fu8fLLLxMREYGdnR1169Zl//79VKtWzVBvIUfDivYoCpy4oyLkTjKVXGwMXZIQQpQqiqJwP73wpyQw0xjpdcy19957j2+++YaKFStiZ2fHzZs36dy5M1OnTkWj0bBw4UK6d+/OxYsXH/t08aRJk/jqq6/4+uuvmTlzJoMGDSIkJAR7e/unrunYsWP07duXiRMn0q9fP/bv389rr72Gg4MDw4cP5+jRo7zxxhv89ttvNG7cmLt377Jnzx5Ad7VqwIABfPXVV/To0YOEhAT27NlDQXb5NWi4Wb58+WO/HxQUlOv1jBkzmDFjRgFW9Ox83WxoUdmRXZdjWLD3Bl/29jN0SUIIUarcT8+k2idbCv285yZ3wNxEf79OJ0+eTLt27XJe29vb4+en+52SlZXFhx9+yKZNm9iwYQNjx4595HGGDx+ecyfk888/54cffuDw4cN07NjxqWv69ttvadOmDR9//DEAVapU4dy5c3z99dcMHz6c0NBQLCwseOGFF7CyssLDw4PatWsDunCTkZFBz5498fDwAKBGjRpPXcPTkHsoevRKc08A1p4IIzJeOt8IIYR4evXq1cv1OjExkQkTJuDj44O9vT3lypXj/PnzhIaGPvY4NWvWzPnawsICa2vrnOkNntb58+dp0qRJrnVNmjTh8uXLZGZm0q5dOzw8PKhYsSJDhgxhyZIlJCcnA+Dn50ebNm2oUaMGffr0YcGCBdy7d++Z6sivIvW0VHFXv4IdnlYK1xPgp73X+aCzj6FLEkKIUsNMY8S5yR0Mcl59srCwyPV6woQJBAYG5tyqyszM5KWXXiItLe2xx/nv00UqlarABv6zsrLi+PHjBAUFsXXrVj755BMmTpzIkSNHsLW1JTAwkP3797N161ZmzpzJhx9+yKFDh/D09CyQeuTKjZ61K6v74Cw5GEJcsoxoKoQQhUWlUmFuYlzoS0HPcbhv3z6GDx9Ojx49qFGjBk5OTty4caNAz/lfPj4+7Nu3L09dVapUyRlJ2NjYmLZt2/LVV19x+vRpbty4wY4dOwDdz6ZJkyZMmjSJEydOYGJiwrp16wqsXrlyo2fVbBW8Xay4EJHA4gM3eKNNZUOXJIQQohirXLkya9eupWvXriiKwgcffFBgV2Cio6M5efJkrnWurq68/fbb1K9fnylTptCvXz8OHDjArFmzmD17NgB//fUX165do3nz5tjZ2bFx40aysrKoWrUqhw4dYvv27bRv3x4nJycOHTpEdHQ0Pj4Fd3dDrtzomUoFrzSrAMAv+66TnJa/MQqEEEKIh/n222+xs7OjcePGdO/endatW1OnTp0COdfSpUupXbt2rmXBggXUqVOHlStXsnz5cqpXr84nn3zC5MmTGT58OKAbhHft2rW0bt0aHx8f5s6dy7Jly/D19cXa2prdu3fTuXNnqlSpwkcffcT06dPp1KlTgbwHkCs3BaKjrzPf7bhKyJ1klh2+yYimBXNPUQghRPE1fPjwnHAAuhGCH/Z4dIUKFXJu72RlZREfH8/bb7+da6iU/96methxYmNjH1vPf59Q/q9evXrRq1evh36vadOmj9zfx8eHzZs3P/bY+iZXbgqAsZGaV5p7AbBwzzXSMkrezK1CCCFEUSXhpoD0qlsWJyst4XEprD8RZuhyhBBCiFJDwk0B0RobMbKZ7nbU3F1XycwqMpOvCyGEECWahJsCNLChBzZmGq7FJLHlbIShyxFCCCFKBQk3BchSa8wwf91Q07ODrhToPBpCCCGE0JFwU8CGN/HETGPEmbB49lyOMXQ5QgghRIkn4aaA2VuYMKCBbtbW2UFXDFyNEEIIUfJJuCkELzf3RGOk4uC1uxwLKdjJwoQQQojSTsJNIXC1MaNH7bIAzJGrN0IIIUSBknBTSF5p4YVKBdvOR3ExIsHQ5QghhBAlloSbQuJVxpJO1V0AuXojhBCl2fDhw1GpVKhUKjQaDc7OzrRr146ff/65wCbE/LeWLVsyfvz4Aj+PIUm4KUSvtawEwJ+nw7l5N9nA1QghhDCUjh07Eh4ezo0bN9i0aROtWrVi3LhxvPDCC2RkyITLz0vCTSGqXtaGZpUdycxSmLf7qqHLEUIIYSBarRYXFxfKli1LnTp1+OCDD/jjjz/YtGkTixYtytkuNjaWkSNHUqZMGaytrWnbti3BwcEAXLp0CZVKxYULF3Ide8aMGXh5eT1zbWvWrMHX1xetVkuFChWYPn16ru/Pnj2bypUrY2pqirOzM71798753urVq6lRowZmZmY4ODjQtm1bkpKSnrmWZyXhppBlX71ZefQWUQkpBq5GCCFKEEWBtKTCX/Q0QGvr1q3x8/Nj7dq1Oev69OlDVFQUmzZt4tixY9SuXZuAgADu3r1LlSpVqFevHkuWLMl1nCVLljBw4MBnquHYsWP07duX/v37ExwczMSJE/n4449zAtfRo0d54403mDx5MhcvXmTz5s00b94cgPDwcAYMGMBLL73E+fPnCQoKomfPngYZwNa40M9YyjWqaE+d8rYcD43l5703eK+Tt6FLEkKIkiE9GT53K/zzfnAbTCz0cihvb29Onz4NwN69ezl8+DBRUVFotVoAvv76a9atW8fq1at59dVXGTRoELNmzWLKlCmA7mrOsWPH+P3335/p/N9++y1t2rTh448/BqBKlSqcO3eOr7/+muHDhxMaGoqFhQUvvPACVlZWeHh4ULt2bUAXbjIyMujZsyceHrrR+WvUqPFc7fGs5MpNIVOpVDlXb34/GELc/XQDVySEEKKoUBQFlUoFwKlTp0hMTMTBwQFLS0ssLS2xtrYmJCSEa9euAdC/f39u3LjBwYMHAd1Vmzp16uDt/Wz/cT5//jxNmjTJta5JkyZcvnyZzMxM2rVrh4eHBxUrVmTIkCEsWbKE5GRdH1I/Pz/atGlDjRo16NOnDwsWLODePcOM7SZXbgygtbcTVZ2tuBiZwO8HQxjTqpKhSxJCiOJPY667imKI8+rJ+fPn8fT0BCAxMRFXV1eCgoJyvp+VlUViYiLlypUDwMXFhdatW7N06VIaNWrE0qVLGT16tN7q+S8rKyuOHz9OUFAQW7du5ZNPPmHixIkcOXIEW1tbAgMD2b9/P1u3bmXmzJl8+OGHHDp0KOc9FRa5cmMAarWK0S11nb1+3nud+2mZBq5ICCFKAJVKd3uosJcHV1qe144dOwgODqZXr14A1KlTh4iICIyNjalUqVLOUrFiRRwdHXP2GzRoECtWrODAgQNcu3aN/v37P3MNPj4+7Nu3L9e6ffv2UaVKFYyMjAAwNjambdu2fPXVV5w+fZobN26wY8cOQHd3okmTJkyaNIkTJ05gYmLCunXrnrmeZyVXbgzkhZquTA+8yM2791lxJJThTQo31QohhDCc1NRUIiIiyMzMJDIyks2bNzNt2jReeOEFhg4dCkDbtm3x9/cnICCAr776iipVqnDr1i3WrVtHv379aNCgAQA9e/Zk9OjRjB49mlatWuHm9uR+R9HR0Zw8eTLXOldXV95++23q16/PlClT6NevHwcOHGDWrFnMnj0bgL/++otr167RvHlz7Ozs2LhxI1lZWVStWpVDhw6xfft22rdvj5OTE4cOHSI6OhofHx/9Nl4+yJUbAzE2UjOque7qzYI910nPLPiBm4QQQhQNmzdvxtXVlQoVKtCxY0d27tzJDz/8wB9//JFzhUSlUrFx40aaN2/Oiy++SJUqVRg4cCA3b97E2dk551hWVlZ07dqVU6dOMWjQoHydf+nSpdSuXTvXsmDBAurUqcPKlStZvnw51atX55NPPmHy5MkMHz4cAFtbW9auXUvr1q3x8fFh7ty5LFu2DF9fX6ytrdm9ezedO3emSpUqfPTRR0yfPp1OnTrpvf2eRKUY4hktA4qPj8fGxoa4uDisra31euz09HQ2btxI586d0Wg0T9w+JT2Tpl/uJCYxlW/6+NG7bjm91lNcPW07ioeTdtQPaUf90Hc7pqSkcP36dTw9PTE1NdVDhcVDVlYW8fHxWFtbo1aXvOsTj/u5Ps3v75LXMsWIqcaIEU11t6Pm7rpKVlapyplCCCFEgZBwY2CDG5XHytSYK1GJbD0XaehyhBBCiGJPwo2BWZlqGOZfAdBNqFnK7hIKIYQQeifhpgh4sUkFTDVqTt2KY//VO4YuRwghhCjWJNwUAQ6WWvrXLw/A7KArBq5GCCGKD7naXbLo6+cp4aaIeLl5RYzVKvZducPJm7GGLkcIIYq07Ceusof+FyVD9s/zeZ+ok0H8ioiytmZ0r1WWNcdvMXvnFeYPrWfokoQQosgyMjLC1taWqKgoAMzNzXPmZCrJsrKySEtLIyUlpUQ9Cq4oCsnJyURFRWFra5sz1s+zknBThIxuWZG1J26x9VwklyMTqOxsZeiShBCiyHJxcQHICTilgaIo3L9/HzMzsxIZ5mxtbXN+rs9Dwk0RUsnJivbVnNlyNpI5u67ybd9ahi5JCCGKLJVKhaurK05OTqSnpxu6nEKRnp7O7t27ad68eYkbVFKj0Tz3FZtsBg03EydOZNKkSbnWVa1alQsXLjxyn1WrVvHxxx9z48YNKleuzJdffknnzp0LutRC81rLSmw5G8mGk7d5q10Vytnpb7ZZIYQoiYyMjPT2S7GoMzIyIiMjA1NT0xIXbvTJ4DfsfH19CQ8Pz1n27t37yG3379/PgAEDGDFiBCdOnCAgIICAgADOnDlTiBUXLD93W5pUciAjS2HB7muGLkcIIYQodgweboyNjXFxcclZ/j2N+399//33dOzYkXfeeQcfHx+mTJlCnTp1mDVrViFWXPDGtKwEwPIjN4lJTDVwNUIIIUTxYvA+N5cvX8bNzQ1TU1P8/f2ZNm0a5cuXf+i2Bw4c4K233sq1rkOHDqxfv/6Rx09NTSU19Z+AEB8fD+juW+r7Hm328Z73uPXKW1OznDWnb8Xz0+6rvNWusj7KKzb01Y6lnbSjfkg76oe0o36U5nZ8mvds0FnBN23aRGJiIlWrViU8PJxJkyYRFhbGmTNnsLLK+6SQiYkJixcvZsCAATnrZs+ezaRJk4iMfPi8TA/r1wO66d7NzYtuf5bTd1X8dNEIMyOFiXUyMTV4DBVCCCEMJzk5mYEDB+ZrVnCD/srs1KlTztc1a9akYcOGeHh4sHLlSkaMGKGXc7z//vu5rvbEx8fj7u5O+/btn9g4Tys9PZ3AwEDatWv33B29OmYp7Jq1nyvRSUTb+vBKc089VVn06bMdSzNpR/2QdtQPaUf9KM3tmH3nJT+K1PUAW1tbqlSpwpUrD5+CwMXFJc8VmsjIyMc+E6/VatFqtXnWazSaAvtg6OvYo1tW4u1Vp1h0IJSRzb0w1ZSOpwGyFeTPqDSRdtQPaUf9kHbUj9LYjk/zfg3eofjfEhMTuXr1Kq6urg/9vr+/P9u3b8+1LjAwEH9//8Ior9B1q+VGWVszYhJTWXX0pqHLEUIIIYoFg4abCRMmsGvXLm7cuMH+/fvp0aMHRkZGOX1qhg4dyvvvv5+z/bhx49i8eTPTp0/nwoULTJw4kaNHjzJ27FhDvYUCpTFSM6p5RQDm7b5GRmaWgSsSQgghij6Dhptbt24xYMAAqlatSt++fXFwcODgwYOUKVMGgNDQUMLDw3O2b9y4MUuXLmX+/Pn4+fmxevVq1q9fT/Xq1Q31Fgpc33ruOFiYcOveff48fdvQ5QghhBBFnkH73Cxfvvyx3w8KCsqzrk+fPvTp06eAKip6zEyMeKmpJ19vucicoKt09yuLWl3y5hMRQggh9KVI9bkRDze4kQdWWmMuRSay/ULpmSBOCCGEeBYSbooBGzMNg/09AJgddAUDDk0khBBCFHkSboqJl5p4ojVWcyI0loPX7hq6HCGEEKLIknBTTJSx0tK3njugu3ojhBBCiIeTcFOMjGpeESO1ij2XYwi+FWfocoQQQogiScJNMeJub043PzcA5uySqzdCCCHEw0i4KWZGt/QCYNOZCK5EJRq4GiGEEKLokXBTzFRxtqKtjzOKAvN2XTV0OUIIIUSRI+GmGHqtle7qzboTYdyOvW/gaoQQQoiiRcJNMVSnvB3+FR3IyFJYsOeaocsRQgghihQJN8VU9tWb5YdvcjcpzcDVCCGEEEWHhJtiqmklR2qUteF+eiaL9l03dDlCCCFEkSHhpphSqVS89uDJqUX7b5CYmmHgioQQQoiiQcJNMdbB14WKZSyIT8lg6aEQQ5cjhBBCFAkSbooxtVrFqy10V28W7rlOakamgSsSQgghDE/CTTEXUKssrjamRCWksuZYmKHLEUIIIQxOwk0xZ2Ks5uVmFQGYu+sqGZlZBq5ICCGEMCwJNyVA/wbu2JlrCL2bzN/B4YYuRwghhDAoCTclgLmJMS818QRgTtBVFEUxcEVCCCGE4Ui4KSGG+lfAwsSICxEJ7LwYZehyhBBCCIORcFNC2JhrGNzIA4DZO2VCTSGEEKWXhJsSZERTT0yM1RwNucfh63cNXY4QQghhEBJu9Eh1ZRvGmYabpdvJ2pTedcsBMDvoisHqEEIIIQxJwo2+HP0ZoxUDqHd9FmSmG6yMV5pXRK2CoIvRnL0dZ7A6hBBCCEORcKMvrn6gMcM5IRijjW+BgZ5Y8nCw4IWaboDuySkhhBCitJFwoy9l65LZYyEKKtSnl0HQNIOVMvrBhJobg8O5HpNksDqEEEIIQ5Bwo0dK5facch+ue7HrSzi22CB1+Lha09rbiSwF5u2SqzdCCCFKFwk3ehbi2IrMJm/rXvz1JlzaapA6xrTSXb1Zc/wWEXEpBqlBCCGEMAQJNwUgq8V74DcAlExYNQzCjhd6DXU97GngaU96psLCPdcK/fxCCCGEoUi4KQgqFXT9ASq2gvRkWNoX7l4v9DJee9D3ZunhUO4lpRX6+YUQQghDkHBTUIxNoO+v4FIDkqJhSW9IulOoJbSoUgZfN2uS0zJZfOBGoZ5bCCGEMBQJNwXJ1BoGrgIbd7hzBZb1h/TCG+RPpVLlPDm1aP8NklIzCu3cQgghhKFIuClo1q4waDWY2sCtw7BmJGRlFtrpO1V3xdPRgtjkdJYdDi208wohhBCGIuGmMDh5Q/9lYGQCF/6Cze8V2iB/RmoVrzSvCMDCPddJzSi8YCWEEEIYgoSbwlKhCfSYp/v68HzYP7PQTt2jTlmcrbVExKew/kRYoZ1XCCGEMAQJN4Wpek9oP1X3deDHELy6UE6rNTbi5Wa6qzdzd10jM8swU0MIIYQQhaHIhJsvvvgClUrF+PHjH7nNokWLUKlUuRZTU9PCK1IfGo+FRq/pvl4/Gq7vKZTTDmhQHltzDddjkth0JrxQzimEEEIYQpEIN0eOHGHevHnUrFnzidtaW1sTHh6es4SEhBRChXrWfipU6w6ZabB8EESeK/BTWmiNGd64AgCf/HGWixEJBX5OIYQQwhAMHm4SExMZNGgQCxYswM7O7onbq1QqXFxcchZnZ+dCqFLP1GroMR/K+0NqnG4MnPjbBX7akc0qUqOsDXeT0hi44CCXIyXgCCGEKHmMDV3AmDFj6NKlC23btuWzzz574vaJiYl4eHiQlZVFnTp1+Pzzz/H19X3k9qmpqaSmpua8jo+PByA9PZ309PTnfwP/kn28/B3XCHr/ivHizqjuXEb5vTcZQ/7UjY1TQLRq+HloHYYtOsq58AQGLDjIby/Wo5KTZYGd81k8XTuKR5F21A9pR/2QdtSP0tyOT/OeVYpSSM8kP8Ty5cuZOnUqR44cwdTUlJYtW1KrVi2+++67h25/4MABLl++TM2aNYmLi+Obb75h9+7dnD17lnLlyj10n4kTJzJp0qQ865cuXYq5ubk+384zMUuNpvmlyZhmxBFtWY0DXhNQ1AWbOZPS4cdzRoQlq7DWKIz1zcTZrEBPKYQQQjyX5ORkBg4cSFxcHNbWj78QYLBwc/PmTerVq0dgYGBOX5snhZv/Sk9Px8fHhwEDBjBlypSHbvOwKzfu7u7ExMQ8sXGeVnp6OoGBgbRr1w6NRpP/HcNPYfx7N1RpSWRV70Nmt9m6+akK0L3kNIb+fJQLkYk4WWn5/aV6eDpaFOg58+uZ21HkIu2oH9KO+iHtqB+luR3j4+NxdHTMV7gx2G2pY8eOERUVRZ06dXLWZWZmsnv3bmbNmkVqaipGRkaPPYZGo6F27dpcuXLlkdtotVq0Wu1D9y2oD8ZTH7t8Pd08VEv6oj6zCrVdeWjzSYHUls3JRsPSUf4MXHCQCxEJDPnlKMtH+ReZgAMF+zMqTaQd9UPaUT+kHfWjNLbj07xfg3UobtOmDcHBwZw8eTJnqVevHoMGDeLkyZNPDDagC0PBwcG4uroWQsUFrFJb6PaD7us90+HITwV+SnsLE5aMbEgVZ0si41MZMP8gN2KSCvy8QgghREEyWLixsrKievXquRYLCwscHByoXr06AEOHDuX999/P2Wfy5Mls3bqVa9eucfz4cQYPHkxISAgjR4401NvQr9qDoeUHuq83ToALGwv8lA6WWpa+3IjKTpZExKcwYMFBQu8kF/h5hRBCiIJi8EfBHyc0NJTw8H8GnLt37x4vv/wyPj4+dO7cmfj4ePbv30+1atUMWKWetfgf1B4CShasfgluHS3wUzo+CDheZSwIj9MFnJt3JeAIIYQongz+KPi/BQUFPfb1jBkzmDFjRuEVZAgqFbwwAxIi4EogLO0LIwLBwatAT1vGSsuylxvRf8FBrkUn0X/+QZaPaoS7veGfKBNCCCGeRpG+clNqGWmgzyJw9YPkO/B7L0iKKfDTOlmbsuzlRng6WhAWe58BCw4SFnu/wM8rhBBC6JOEm6JKawkDV4Ftebh3XXcFJ63gbxU5Pwg4FRzMuXXvPgPmH+S2BBwhhBDFiISboszKGQavBTM7CDum64OTmVHgp3WxMWXZqEZ4OJgTejeZAQsOEh4nAUcIIUTxIOGmqHOsDANWgLEpXNoEm/4HhTDuoquNGctebkR5e3NC7iQzcMEhIuJSCvy8QgghxPOScFMclG8IPRcAKjj6E+z9tlBO62ZrxrJRjShnZ8b1mCQGLjhIVLwEHCGEEEWbhJviolo36PSl7uvtk+HU8kI5bVlb3RWcsrZmXItJov+Cg0QlSMARQghRdEm4KU4avgKNX9d9/ccYuLqzUE7rbm/O8lEPAk50EgPmHyQ6IfXJOwohhBAGIOGmuGk7Gar3gqwMWDEEIoIL5bTu9uYse7kRrjamXI3W3aKKSZSAI4QQouiRcFPcqNUQMAc8mkJaAizpA3G3CuXU5R10V3BcrE25HJXIwAUHuSMBRwghRBEj4aY4MtZC/yVQxgcSwuH33nA/tlBO7eFgwbJRjXC21nIpMpFBCw9xNymtUM4thBBC5IeEm+LKzBYGrQIrV4g+D8sHQUbhXEXxdLRg2cuNcLLSciEigYELDnJPAo4QQogiQsJNcWbrrgs4JlYQshfWj4asrEI5dcUylix9uRFlHgScQQsPEZssAUcIIYThSbgp7lxqQL/fQG0MZ9bAtk8L7dSVnCxZ9nJDHC21nAuPZ/BPh4hLTi+08wshhBAPI+GmJPBqBd1/1H29/wc4NK/QTl3JyYplLzfEwcKEM2EPAs59CThCCCEMR8JNSeHXH9p8ovt607tw/s9CO3VlZyuWvtwIewsTgsPiGPrTIeJTJOAIIYQwDAk3JUnTt6DeS4ACa0ZC6KFCO3VVFyuWvtwQO3MNp27FMfSnwyRIwBFCCGEAEm5KEpUKOn0NVTpBRgos6wcxlwvt9N4u1iwZ2Qhbcw0nb8Yy7GcJOEIIIQqfhJuSxsgYev8EZevC/Xvwey9IiCy001dzs2bJyIbYmGk4HhrL8F+OkJiaUWjnF0IIISTclEQmFjBgBdh5QmwILO0LqYmFdnpfNxuWjGyItakxx0Lu8eIvh0mSgCOEEKKQSLgpqSzLwOA1YO4A4Sdh1XDILLyAUb2sDUtGNsLK1JgjN+7x4qIjJKdJwBFCCFHwJNyUZA5eMHAlGJvBlUD4+01QlEI7fY1yNvw+oiFWWmMOX7/LSxJwhBBCFAIJNyVduXrQ+2dQqeH4r7D760I9vZ+7Lb+OaICV1piD1+4yYtFR7qdlFmoNQgghShcJN6WBd2fo/I3u651T4eCcQr2CU7u8HYteaoCl1pgD1+7w8q9HSUmXgCOEEKJgSLgpLeqP0I2DA7D5PVg5VPc0VSGp62HH4pfqY2FixN4rMRJwhBBCFBgJN6VJm0+g/Weg1sD5DTCnKYQcKLTT1/WwZ9FLDTA3MWLP5RhG/XZMAo4QQgi9k3BTmqhU0Ph1GLEV7CtC/C1Y1BmCvoSswgkZ9SvY88vw+phpjNh9KZpXfz9GaoYEHCGEEPoj4aY0KlsHXtkNfgNAyYKgz2FxV4gLK5TTN6zowC8v6gJO0MVoRv9+XAKOEEIIvZFwU1ppraDHXOgxH0wsIWQfzG0C5/8qlNM3qujAT8PrYapRs+NCFGOWHCctI6tQzi2EEKJkk3BT2vn1013Fcaut62C8YhD8/Tak3y/wUzf2cuSnYfXRGqvZdj6KMUsl4AghhHh+Em6EbrC/l7ZC4zd0r48shAWtIep8gZ+6SSVHFg6rh4mxmsBzkby+7DjpmRJwhBBCPDsJN0LH2ATaT4HBa8HCCaLOwfxWcPSXAh8Tp1nlMiwYqgs4W85G8ubK00i+EUII8awk3IjcKrWB0fvAqw1k3Ie/xhfKmDgtqpRh/pC6mBip2XIuikWX1aTKLSohhBDPQMKNyMvSCQatLvQxcVpWdWLekLpojFScvqvmld9PyGziQgghnpqEG/FwarVBxsRp5e3ET0PrYKJW2Hf1DoN/OkRccnqBnU8IIUTJI+FGPJ4BxsTxr+jA2GqZ2JppOBEaS7/5B4hKSCmw8wkhhChZiky4+eKLL1CpVIwfP/6x261atQpvb29MTU2pUaMGGzduLJwCSzMDjInjYQVLRtTDyUrLhYgE+sw9wM27yQV2PiGEECVHkQg3R44cYd68edSsWfOx2+3fv58BAwYwYsQITpw4QUBAAAEBAZw5c6aQKi3lCnlMnCrOVqx+tTHu9maE3Emm99z9XI5MKJBzCSGEKDkMHm4SExMZNGgQCxYswM7O7rHbfv/993Ts2JF33nkHHx8fpkyZQp06dZg1a1YhVSsKe0yc8g7mrH61MVWcLYmMT6XvvAOcvhVbIOcSQghRMhg83IwZM4YuXbrQtm3bJ2574MCBPNt16NCBAwcKb2ZrQaGPieNsbcqKUf74udtyLzmdgQsOceDqHb2fRwghRMlgbMiTL1++nOPHj3PkyJF8bR8REYGzs3Oudc7OzkRERDxyn9TUVFJTU3Nex8fHA5Cenk56un6fwsk+nr6PW2R5NIeRQRj9ORb1tR3w13iyrmwns/MMMLN95sM+rB0tTVQsGlaH15ae5MC1uwz75TA/9KtJG2+n530XJVap+zwWEGlH/ZB21I/S3I5P854NFm5u3rzJuHHjCAwMxNTUtMDOM23aNCZNmpRn/datWzE3Ny+QcwYGBhbIcYss66F4uTlTLXwl6gt/knJ1H8cqjOauZdXnOuzD2rFXGUi8pyb4Hry25ASDKmVRr0zBjqBc3JW6z2MBkXbUD2lH/SiN7ZicnP+HSlSKUsBj6z/C+vXr6dGjB0ZGRjnrMjMzUalUqNVqUlNTc30PoHz58rz11lu5nqj69NNPWb9+PadOnXroeR525cbd3Z2YmBisra31+p7S09MJDAykXbt2aDQavR67OFDdPoHR+lGo7l1HUanJavYOWU3eArXRk3f+lye1Y0ZmFh+sP8u6k+GoVPBpF28GNSyvr7dRYpT2z6O+SDvqh7SjfpTmdoyPj8fR0ZG4uLgn/v422JWbNm3aEBwcnGvdiy++iLe3N++++26eYAPg7+/P9u3bc4WbwMBA/P39H3kerVaLVqvNs16j0RTYB6Mgj12keTSAV/fA3xNQnV6O0e4vMQrZCz3ng025pz7co9pRo4HpfWtjY65l0f4bTPzrAknpCq+19EKlUunjnZQopfbzqGfSjvoh7agfpbEdn+b9GizcWFlZUb169VzrLCwscHBwyFk/dOhQypYty7Rp0wAYN24cLVq0YPr06XTp0oXly5dz9OhR5s+fX+j1i0fQWkHPeeDVSveYeMg+mNMEus8Cn656O41areLTrtWwNtPww/bLfL3lInH303m/k7cEHCGEKOUM/rTU44SGhhIeHp7zunHjxixdupT58+fj5+fH6tWrWb9+fZ6QJIoAv/7/jImTEgsrBsNfb+l1TByVSsVb7arw8QvVAJi/+xrvrw0mM0v64AghRGlm0Kel/isoKOixrwH69OlDnz59Cqcg8Xyyx8TZMRn2z4SjP0HoAej9Mzj56O00I5p6YmVqzHtrTrP8yE0SUjKY0a8WJsZFOrsLIYQoIPKvvyhYxia62cUHrwGLMg/GxGkJR3/W65g4feu5M3tQHUyM1PwdHM7IX4+SnCYzigshRGkk4UYUjkptYfR+8GoDGSnw15uwcggk39XbKTpWd+Wn4fUw0xix+1I0Q386TNz90jcWhBBClHYSbkThsXSCQat1V3LUGjj/J8xtBiH79XaKZpXL8PvIhlibGnM05B795x8kOiH1yTsKIYQoMSTciMKlVkPj12HEVrCvCPG3YFEXCPoSsjL1coq6HnaseMUfR0st58Pj6TvvALfuyYziQghRWki4EYZRto7uaaqa/UHJgqDPYXFXiLull8P7uFqz+lV/ytqacT0miT5zD3AlKlEvxxZCCFG0SbgRhpM9Jk6PeWBimTMmjurC33o5fAVHC9aMbkwlJ0vC41LoO+8AZ8Li9HJsIYQQRZeEG2F4/xkTx3jNMGreXKzrePycXGxMWfmKPzXK2nA3KY0B8w9y6JrMKC6EECWZhBtRNGSPidP4DQA8Y7Zj/EsHiL703Ie2tzBh6csNaehpT0JqBkN/PszOC1HPfVwhhBBFk4QbUXQYm0D7KWT0X0GqsRWqqLMwvwWcWPLcY+JYmWpY/FID2ng7kZqRxcu/HmXDqdt6KlwIIURRIuFGFDmKVxt2ek8lq0IzSE+GP16DtaMgNeG5jmuqMWLukLp0r+VGRpbCuOUnWHIoRE9VCyGEKCok3IgiKVVjS+aA1dD6I1AZQfBKmNccbp98ruNqjNTM6FuLIY08UBT4cN0ZZgdd0U/RQgghigQJN6LoUhtB83dg+N9gXQ7uXoOf2sHBuc91m0qtVjG5uy9jWnkB8NXmi3yx6QKKHqeDEEIIYTgSbkTR5+EPr+4B7xcgMw02vwvLBjzX1A0qlYp3OnjzQWdvAObuusqH68/IjOJCCFECSLgRxYO5PfT7HTp9DUYmcGkTzG363FM3jGruxRc9a6BSwdJDoYxbfoK0jCw9FS2EEMIQninc3Lx5k1u3/hlJ9vDhw4wfP5758+frrTAh8lCpoOEoGLkN7L0gPkw3dcOur55r6ob+Dcoza0AdNEYq/jodzqjfjnI/TT9TQQghhCh8zxRuBg4cyM6dOwGIiIigXbt2HD58mA8//JDJkyfrtUAh8nD1g1d2/TN1w86p8Gt3iA9/5kN2qenKgqH1MNWoCboYzbCfDxOfIjOKCyFEcfRM4ebMmTM0aNAAgJUrV1K9enX279/PkiVLWLRokT7rE+LhsqduCJgLGgu4sQfmNoHLgc98yJZVnfhtREOsTI05fOMuA+Yf5E6izCguhBDFzTOFm/T0dLRaLQDbtm2jW7duAHh7exMe/uz/exbiqdUaoLuK41wDku/Akt6w5UPISHumw9WvYM/yUY1wsDDh7O14+sw7QFjsfT0XLYQQoiA9U7jx9fVl7ty57Nmzh8DAQDp27AjA7du3cXBw0GuBQjyRY2VdP5wGo3SvD8yCnzvA3evPdDhfNxtWveqPm40p16KT6DNnP9eiZUZxIYQoLp4p3Hz55ZfMmzePli1bMmDAAPz8/ADYsGFDzu0qIQqVxhQ6fw39loCpLdw+rhv078yaZzpcxTKWrB7dmIplLLgdl0KfuTKjuBBCFBfPFG5atmxJTEwMMTEx/PzzzznrR40axdy5c/VWnBBPzecFeHUvuDeC1HhY/RJseAPSkp/6UG62Zqx8xR9fN2vuPJhR/MiNZx9bRwghROF4pnBz//59UlNTsbOzAyAkJITvvvuOixcv4uTkpNcChXhqtu66UY2bTQBUcHwxLGgFkeee+lCOllqWjWpEgwq6GcWH/HSIoIsyo7gQQhRlzxRuunfvzq+//gpAbGwsDRs2ZPr06QQEBDBnzhy9FijEMzEyhjYfw9D1YOkM0Rd0AefYoqeeusH6wYziraqWISVdN6P4X6dlRnEhhCiqnincHD9+nGbNmgGwevVqnJ2dCQkJ4ddff+WHH37Qa4FCPJeKLXW3qbzaQEYK/DkOVr8IKU/Xf8bMxIh5Q+rR1c+N9EyF15ed4NcDN2Q+KiGEKIKeKdwkJydjZWUFwNatW+nZsydqtZpGjRoREhKi1wKFeG6WTjBoNbSbDGpjOLsO5jaDW8ee6jAmxmq+61eLgQ3LoyjwyR9n+WBdMKkZMpqxEEIUJc8UbipVqsT69eu5efMmW7ZsoX379gBERUVhbW2t1wKF0Au1GpqMgxc3g215iA2Bn9vDvh8gK/9zSRmpVUwNqM67Hb1RqWDZ4ZsMXHCIqPiUAixeCCHE03imcPPJJ58wYcIEKlSoQIMGDfD39wd0V3Fq166t1wKF0Cv3+vDKHqjWHbIyIPBjWNoHEqPzfQiVSsXoll78Mrw+VqbGHAu5R9dZezl5M7bg6hZCCJFvzxRuevfuTWhoKEePHmXLli0569u0acOMGTP0VpwQBcLMFvoshhdmgLEpXNmmm2H82q6nOkzLqk5sGNuUSk6WRMan0nfeAVYdvVkwNQshhMi3Zwo3AC4uLtSuXZvbt2/nzBDeoEEDvL299VacEAVGpYJ6L8HLO8CxKiRG6Cbf3PEZZGbk+zCejhase60xbX2cScvI4p3Vp5m44Szpmfm/1SWEEEK/nincZGVlMXnyZGxsbPDw8MDDwwNbW1umTJlC1lP0XxDC4Jx9YdROqD0EUGD317D4BYi7le9DWJlqmD+kLuPaVAZg0f4bDP3pMHeTnm1+KyGEEM/nmcLNhx9+yKxZs/jiiy84ceIEJ06c4PPPP2fmzJl8/PHH+q5RiIJlYgHdZ0Gvn8DECkIPwJwmcOHvfB9CrVbxZrsqzBtSFwsTIw5cu0PXmXs5e1umbBBCiML2TOFm8eLFLFy4kNGjR1OzZk1q1qzJa6+9xoIFC1i0aJGeSxSikNToDa/uBrfakBILywfCxv9BRmq+D9HB14V1Y5pQwcGcsNj79Jqznz9PyYB/QghRmJ4p3Ny9e/ehfWu8vb25e1fm3hHFmH1FeGkr+I/VvT48Dxa2hTtX832IKs5W/DGmKc2r6EY0fn3ZCb7YdIHMLBnwTwghCsMzhRs/Pz9mzZqVZ/2sWbOoWbPmcxclhEEZm0CHqTBwJZjZQ8Rp3Qzjp1bk+xA25hp+GV6fV1pUBGDurqu8tOgIccnpBVW1EEKIB4yfZaevvvqKLl26sG3btpwxbg4cOMDNmzfZuHGjXgsUwmCqdIDR+2DNyxCyF9aNgmtB0Plr0Fo+cXcjtYr3O/ng62bD/1afYtelaLr/uJcFQ+tR2dmq4OsXQohS6pmu3LRo0YJLly7Ro0cPYmNjiY2NpWfPnpw9e5bffvtN3zUKYTjWbjBsA7T8AFRqOLUU5reEiOB8H6KbnxurX21MWVszbtxJJuDHfWw9G1FwNQshRCn3zOPcuLm5MXXqVNasWcOaNWv47LPPuHfvHj/99FO+jzFnzhxq1qyJtbU11tbW+Pv7s2nTpkduv2jRIlQqVa7F1NT0Wd+CEPmjNoKW78KwP8HKFe5chgVtYNfXkJqYr0NUL2vDhrFNaFTRnqS0TEb9dozvtl0iS/rhCCGE3j1zuNGHcuXK8cUXX3Ds2DGOHj1K69at6d69O2fPnn3kPtbW1oSHh+csMlGnKDQVmsKr+6ByB8hMhZ2fwQ+14OAcSH/y3FIOllp+G9GQ4Y0rAPDdtsu8+vsxElPzP2igEEKIJzNouOnatSudO3emcuXKVKlShalTp2JpacnBgwcfuY9KpcLFxSVncXZ2LsSKRaln4QADV0DPBWBXAZKiYfN7MLMuHFv8xNGNNUZqJnbz5aveNTExUrP1XCQ9ftzHjZikwqlfCCFKgWfqUFwQMjMzWbVqFUlJSTmdlB8mMTERDw8PsrKyqFOnDp9//jm+vr6P3D41NZXU1H/GKYmPjwcgPT2d9HT9PrmSfTx9H7e0KRbt6NMDqryA6tRSjPZ+gyr+Fvz5Bsq+78hs/i5KtR66PjqP0MPPBU97U8YuO8XlqES6zdrLjL41aV7ZUW8lFot2LAakHfVD2lE/SnM7Ps17VimKku+b/j179nzs92NjY9m1axeZmZn5LiA4OBh/f39SUlKwtLRk6dKldO7c+aHbHjhwgMuXL1OzZk3i4uL45ptv2L17N2fPnqVcuXIP3WfixIlMmjQpz/qlS5dibm6e7zqFeBR1VhoVYnZQJfJPtBkJAMSZunPerTeR1rV081g9Qlwa/HzRiBuJKlQodC2fRWs35XG7CCFEqZScnMzAgQOJi4vD2tr6sds+Vbh58cUX87XdL7/8kt9DkpaWRmhoKHFxcaxevZqFCxeya9cuqlWr9sR909PT8fHxYcCAAUyZMuWh2zzsyo27uzsxMTFPbJynlZ6eTmBgIO3atUOj0ej12KVJsW3H1ATUR+ajPjgLVaou5GS51SWr1YcoFZo/ereMLCb/dZ6Vx8IA6FLDhWkBvpiZGD1XOcW2HYsYaUf9kHbUj9LcjvHx8Tg6OuYr3DzVbamnCS35ZWJiQqVKlQCoW7cuR44c4fvvv2fevHlP3Fej0VC7dm2uXLnyyG20Wi1arfah+xbUB6Mgj12aFLt21NhDq/eg4SjY/wMcnIv69jHUS3qCZ3No/Qm418+7mwa+7O1HjXK2TPrzHH8HR3A9Jpl5Q+ribv/8VxeLXTsWUdKO+iHtqB+lsR2f5v0atEPxw2RlZeW60vI4mZmZBAcH4+rqWsBVCfEUzO2h7UQYdwoavAJqDVzfDT+1haX9IeJMnl1UKhVD/CuwZGRDHCxMOBceT/cf93Hg6p3Cr18IIYo5g4ab999/n927d3Pjxg2Cg4N5//33CQoKYtCgQQAMHTqU999/P2f7yZMns3XrVq5du8bx48cZPHgwISEhjBw50lBvQYhHs3KGzl/B68eg1mBdB+NLm2BuU1j9EsTkveLYsKIDf77elOplrbmblMbgnw6xaN91nuLusRBClHoGDTdRUVEMHTqUqlWr0qZNG44cOcKWLVto164dAKGhoYSHh+dsf+/ePV5++WV8fHzo3Lkz8fHx7N+/P1/9c4QwGDsPCPgRXjsEvj0ABc6sgR8bwB9jIfZmrs3dbM1Y/WpjAmq5kZmlMPHPc7yz+jQp6fnvqC+EEKWZQR8Ff9JoxkFBQblez5gxgxkzZhRgRUIUoDJVoM8iaPoW7PgMLm+BE7/B6RVQ7yVo9jZYOgFgqjFiRr9a+LrZMG3TeVYfu8XlqETmDa6Li42Myi2EEI9T5PrcCFHiudaEQSvhpa3g0RQy0+DQXPjeD7ZPhvv3AF0/nJebV2TxSw2wMdNw6mYsXWft5VjIXQO/ASGEKNok3AhhKOUbwvC/YMg6cKsD6cmwZ7ou5Oz+JmfeqmaVy/Dn2KZUdbYiOiGV/vMPsvxwqIGLF0KIokvCjRCGpFKBV2t4eQf0WwJO1SAlDnZMyTVvVXkHc9a+1phO1V1Iz1R4b20wH68/Q1pGlqHfgRBCFDkSboQoClQq8HkBXt0LPReCnee/5q2qA8cWY2GsMHtQHSa0r4JKBb8dDGHwwkNEJ+Rv6AQhhCgtJNwIUZSojaBmHxh7BLp+D1ZuEB8Gf74BPzZAdWYNY1t6sWBIPSy1xhy+cZdus/YSfCvO0JULIUSRIeFGiKLISAN1h8MbJ6DDNDB3gLvXYM0ImNeMtupjrH+tMRUdLQiPS6H33P2sPxFm6KqFEKJIkHAjRFGmMQX/13SjHbf6CLQ2EHkGlg+g0oYA/nwhk9beTqRmZDF+xUk+++scGZnSD0cIUbpJuBGiONBaQYt3YNxJaPomaMwh7CgWy3vyE5OZWu8+AAv3Xmf4L0e4l5Rm2HqFEMKAJNwIUZxkz1v1xkndvFVGJqhu7GbQmREc8lyAn+YWe6/E0O3HvVyMSDB0tUIIYRASboQojv49b1Vt3bxVzuE7+cPofyy0mI3xvWv0XXCYYzEqQ1cqhBCFTsKNEMWZbXno/iOMOQy+PQFom7mXbdp3+CRrDjuu3GXC6mDiU9INXKgQQhQeCTdClASOlaHPL/DKHqjSESOy6G8cxE6Tt2h27lNe+XYZR27ItA1CiNJBwo0QJYlrTRi4AkYEkuXZAo0qk95Gu1mS+gbRP/Xj17UbSJenqYQQJZyEGyFKIvcGZA5cw64qn5Lm1RG1SqGz0WGGnh7CqS/acfv0DkNXKIQQBUbCjRAlWKyFF6r+v8PoA4S5v0AmKuqlH8VtbQ+ifmiNcnkbKIqhyxRCCL2ScCNEaeBcjbIjlhAz/ADbzTuRphjhdPcYqiW9yJjbAs5tgCy5XSWEKBkk3AhRijhX8KHVhGWsbPwXv2R24r5ignHkKVg5BGY3glPLITPD0GUKIcRzkXAjRCmjVqsY3KEx9V+dx2Crn5iZEUC8Yg4xF2HdK7pZyI/8BOkphi5VCCGeiYQbIUqp6mVt+P2NLkTXf4cmqT/wVXo/YlU2EBsCf78F3/vB/pmQmmjoUoUQ4qlIuBGiFDMzMWJy9+p8P7w5K8360Oj+d0zJHEai1hkSI2DrR/BddQj6Eu7fM3S5QgiRLxJuhBC09nZm8/jmNPZ256f0DtSO+5qFdm+SYeupCzVBn8OM6hD4CSRGGbpcIYR4LAk3QggAHC21/DSsHlMCqmOkMeGz8Po0iJvGyYbTwckX0hJh3/fwXQ34ewLEhhq6ZCGEeCgJN0KIHCqViiGNPPjr9WZUL2vN3ftZBOxy5X9lZpPSewmUrQcZKXBkAfxQG9a/BjGXDV22EELkIuFGCJFHJSdL1o5uwqstvFCpYOWxMDpssuBE+1UwdAN4toCsDDi5BGbVh5XDIPy0ocsWQghAwo0Q4hFMjNW818mbpSMb4WZjSsidZHrPO8gP193IGLweRmyDqp0BBc6th3nNYEkfCD1k4MqFEKWdhBshxGP5ezmwaVxzXqjpSmaWwreBl+g3/yA3LXxhwDJ4dR9U7w0qNVzeCj+3h1+6wNUdMrWDEMIgJNwIIZ7IxlzDzAG1+bavH5ZaY46F3KPT93tYc+wWirMv9P4Jxh6FOkNBrYGQvfBbD1jQGs7/JVM7CCEKlYQbIUS+qFQqetYpx6ZxzajnYUdiagZvrzrF2GUniEtOBwcv6DYTxp2ChqPB2AxuH4cVg2BOYzi9UqZ2EEIUCgk3Qoin4m5vzvJRjXi7XRWM1Cr+Ph1Ox+93s/9qjG4Dm7LQ6Qt48ww0exu01hB9Hta+DLPqwtFfICPVsG9CCFGiSbgRQjw1YyM1r7epzJrRjangYE54XAqDFh5i2sbzpGZk6jaycIQ2n+hCTuuPwdwB7t2Av8brpnY48COkJRnybQghSigJN0KIZ1bL3Za/32hG//ruKArM232NHj/u50pUwj8bmdpA8wkw/gx0/AKs3CAhHLZ8oBv1eMuHcOuodD4WQuiNhBshxHOx0BrzRa+azB1cFztzDefC4+nyw15+O3AD5d+BxcQcGo2GcSeh6w9g5wn378KBWbCwDXxX80HQOSZBRwjxXCTcCCH0omN1FzaPb06zyo6kZmTx8R9nGbH4KNEJ/+lfY6yFusN0T1f1Xwo1+oCJJcSFPgg6rXVBZ+vHECZBRwjx9CTcCCH0xtnalMUvNuCTF6phYqxmx4UoOn63m+3nI/NubGQM3l2g10J45wr0+103Xo7GQhd09v+ge5T8+5q6CTvDjkvQEULki4QbIYReqdUqXmrqyYaxTfB2seJOUhojFh/lo/XB3E/LfPhOGjPw6aobL+d/V6Hvb1C9ly7oxIbqJuxc0ErXETnwU7h9UoKOEOKRJNwIIQqEt4s168c0YURTTwB+PxjKCzP3cCYs7vE7asygWjfo/bPuik7fX8G3B2jMITYE9n0H81vAD7Vg20QIPyVBRwiRi0HDzZw5c6hZsybW1tZYW1vj7+/Ppk2bHrvPqlWr8Pb2xtTUlBo1arBx48ZCqlYI8bRMNUZ8/EI1fhvRACcrLVejk+gxex9zgq6SmZWPQGJiDtW6Q59F8M5V6LMYqgXoBgi8dwP2zoB5zXUzlG+bpJu8U4KOEKWeQcNNuXLl+OKLLzh27BhHjx6ldevWdO/enbNnzz50+/379zNgwABGjBjBiRMnCAgIICAggDNnzhRy5UKIp9Gschk2j29OB19n0jMVvtx8gYELDhIWez//BzExB98A6LtYd+uqzyJd8DE2g3vXYe+3usk7Z9aF7ZMhIliCjhCllEHDTdeuXencuTOVK1emSpUqTJ06FUtLSw4ePPjQ7b///ns6duzIO++8g4+PD1OmTKFOnTrMmjWrkCsXQjwtewsT5g6uy5e9amBuYsSh63fp+N1uVh+7lfuR8fwwsdDdqur7q+7WVe9fwKcbGJvC3auwZzrMbQqz6sH2KRBxRoKOEKWIsaELyJaZmcmqVatISkrC39//odscOHCAt956K9e6Dh06sH79+kceNzU1ldTUfx5FjY+PByA9PZ309PTnL/xfso+n7+OWNtKO+lFU27FnLVfquFvz9upgTt+KZ8KqU6w4EsqkF3yo7Gz59AdUa6FqV92Slojq8lbU5zeguroN1Z0rsOcb2PMNikMlsny6k+XTHcr4gEqVr8MX1XYsbqQd9aM0t+PTvGeV8tT/ZdKv4OBg/P39SUlJwdLSkqVLl9K5c+eHbmtiYsLixYsZMGBAzrrZs2czadIkIiMf8qgpMHHiRCZNmpRn/dKlSzE3N9fPmxBCPLXMLNgRrmLLLTXpWSrUKoVWrgodymWhNXr+4xtn3sc57iRusYdxjj+NkfLPP4wJWlfC7Bpy27YBCWblnv9kQogCl5yczMCBA4mLi8Pa2vqx2xo83KSlpREaGkpcXByrV69m4cKF7Nq1i2rVquXZ9lnCzcOu3Li7uxMTE/PExnla6enpBAYG0q5dOzQajV6PXZpIO+pHcWnHsNj7TPn7AtsvRAPgZmPKR529aetTBlU+r648UWoCqstbUJ//A9XVHagy//k3QXGs8uCKTgCUqZpn1+LSjkWdtKN+lOZ2jI+Px9HRMV/hxuC3pUxMTKhUqRIAdevW5ciRI3z//ffMmzcvz7YuLi55QkxkZCQuLi6PPL5Wq0Wr1eZZr9FoCuyDUZDHLk2kHfWjqLdjhTIafhregMBzkUzccJaw2Pu8tuwkbbydmNjNF3d7PVxh1dhD7QG6JSUeLm2Gs+vgyjZUMZcw2vM1Rnu+1t2u8g3Q9ef5T9Ap6u1YXEg76kdpbMeneb9FbpybrKysXFda/s3f35/t27fnWhcYGPjIPjpCiOKjXTVnAt9qzmstvdAYqdh+IYp2M3bx484rpGVk6e9EptZQsy8MWKbrjNxjPlTpBEYmEH0egqbBjw1gtj8EfQkxl/V3biFEoTDolZv333+fTp06Ub58eRISEli6dClBQUFs2bIFgKFDh1K2bFmmTZsGwLhx42jRogXTp0+nS5cuLF++nKNHjzJ//nxDvg0hhJ6Ymxjzv47e9KxTlo/Wn+Hgtbt8veUia4/fYkpAdRp7Oer3hKY24NdPt6TEwcVND67obIeocxB1Dk3Q57TROqNW7YTKbcCzuW4/IUSRZdBwExUVxdChQwkPD8fGxoaaNWuyZcsW2rVrB0BoaChq9T8Xlxo3bszSpUv56KOP+OCDD6hcuTLr16+nevXqhnoLQogCUMnJimUvN2L9yTCm/n2eq9FJDFxwiIBabnzQxQcnK1P9n9TUBvz665b7sTlBR7m6A8vUSDj+i25RGUHZuuDVGrxa6b42Kl23B4Qo6gwabn766afHfj8oKCjPuj59+tCnT58CqkgIUVSoVCp61C5H66rOfLP1Ir8fCmH9ydtsvxDFOx2qMqihB0ZqPXU4/i8zW6g1AGoNICPxLsfW/kB9+0SMrgfBnctw67Bu2fUFmFjpruZ4tdIFHvuK+X7MXAhRMAzeoVgIIR7HxlzDlIDq9K5bjo/WnyE4LI5P/jjLqqO3mNqjOjXL2RZsAVorIm1qk9WhM0YaDcTehGs74epOuBYE9+/Cxb91C4BNefBqqQs6ni3A3L5g6xNC5CHhRghRLPi527J+TBOWHArh6y0XCQ6Lo/uP+xjc0IMJHapiY1ZIt4Zs3aHOUN2SlQURp3RB5+oOuHkI4kLh+K+6BRW41f7nqk65BmBsUjh1ClGKSbgRQhQbRmoVQ/0r0LG6C5//fZ71J2/z28EQNp0J58MuPgTUKqu/sXHyQ63WhRe32tDsLUhLgpD9/4Sd6PNw+7hu2TMdNBZQoYku6FRspXvcXG5hCaF3Em6EEMWOk5Up3/WvTd/67ny8/gxXo5N4c8UpVhy5yWcB1ankZGWYwkwsoHI73QIQH667dXV1h+5WVlI0XN6qWwCs3HRXdSq2gootwbKMYeoWooSRcCOEKLYaezmyaVxzFuy5xg/bL3Pw2l06fb+Hl5tV5PXWlTEz0cM8Ds/D2jWnYzJZWRB19p+rOqEHIOE2nFyiWwBcavxzVae8P2gK4KkwIUoBCTdCiGLNxFjNmFaV6ObnxsQNZ9l+IYrZQVf54+RtJnXzpW01Z0OXqKNW68KLSw1o8gak34fQg/9c1YkI/mfZ971uhnOPxrqg49UanH3lFpYQ+SThRghRIrjbm7NwWD0Cz0Uy6c9zhMXeZ+SvR2nr48zEbtUoZ1fEJsrVmD3oaNxK9zoxCq7t+ifsJITrvr66AwI/Bgsn3a0rr9a6P61dDVm9EEWahBshRImhUqlo7+tC08qO/LD9Cgv3XGPb+Uj2XYnhjTaVGdHUExPjIjfrjI6lE9Tso1sUBaIv/hN0buyFpCgIXqlbAJyqPbiq00p3hcfEwrD1C1GESLgRQpQ45ibGvNfpn2kcDl+/y5ebL+RM49CoooOhS3w8lQqcvHWL/2uQkQo3Dz8YX2cH3D6ZMz0EB3/UzYvlUhPK1gG3OrpRkx0q6W6FCVEKSbgRQpRYVZytWDGqEWuPh/H5xvNcjkqk//yD9Kxdlg+6+OBoqTV0ifljrAXPZrqlzSeQfFf3FFb2YIJxNyHsqG7JprUGV7/cgcemnPTbEaWChBshRImmUqnoVbccbXyc+HrLRZYeDmXtiTC2nY/knY7eDGxQvuCmcSgo5vZQvaduURS4ew3CHoynE3Ycwk9Bajzc2KNbslmUeRB06vzzp4WeJyMVogiQcCOEKBVszU2Y2qNGzjQOZ2/H8/H6M6w+epOpPWpQvWwxnelbpQIHL91S88G8e5kZEH0Bwo79E3iizj0YZ2eLbslmWz534HGrBVoDjRMkhJ5IuBFClCq1y9vxx5gm/H4whOlbL3HqVhzdZu1lSCMP3u5QFWvTEjDDt5ExuFTXLXWH6dal34eIM7kDz53LEBuqW86tf7CzSjdy8r8Dj0t13a0xIYoJCTdCiFLH2EjN8CaedK7hymd/n2fDqdssPhDC38ERfPyCD9383Ap3GofCoDED9/q6JVtKnK5zck7gOQHxt3RXfaIvwKmluu3UGl3A+XfgKVMV1AYeJFGIR5BwI4QotZysTflhQG361nPnkz/OcC0miXHLT7Ly6E0md6+OVxlLQ5dYsExtoGIL3ZItMUp3VeffV3ju34XbJ3TL0Z9022ks/umwnB147CpIh2VRJEi4EUKUek0rO7JpfDPm77rGrJ1X2HflDh2/280rzb0Y1dTD0OUVLksnqNpRt4Cuw3JsyL8Czwnd1Z70JAjdr1uymdnrJhEtW/efwGNVREaIFqWKhBshhAC0xka83qYy3WuV5dMNZ9h5MZpZO6+w/sQt2jmp6KQohi7RMFQq3RUZuwq6p7MAsjIh5lLuKzwRZ3RXeK5u1y3ZrMuCW23ULrVwiU2AaC8oU0nmzRIFSsKNEEL8S3kHc34eXp8tZyOY9Oc5bsWm8EusEUfnHeJ/Hb1pWsmx5PXHeVpqI3Dy0S21B+nWZaRC5JkHj6Sf0IWe6IsQHwbxYRhd+IuGAPO/B1S60ONQEewrgr3Xgz8rgr2nrn+QEM9Bwo0QQvyHSqWiY3VXmlUuw487LrNwz1WCw+IZ8tNhGlW0538dvalT3s7QZRYtxtoHt6Pq/rMuNUE35k7YcbLCjhN//QQ2mTGo0hJ1HZfjb8H13XmPZV32X2HnweLgBXaeYFLE5ggTRZKEGyGEeAQLrTFvtq2EW9IlLmsqsuzwLQ5eu0vP2ftp6+PMhA5V8HaxNnSZRZfWCio0hQpNyUxPZ9fGjXTu1AlNWpxu4MG71+Du1X++vnMNUuNyrvbkGoAwm5Xrgys9nrrA8+8AJPNriQck3AghxBNYaeCjzt6MalGJ77ddYvWxW2w7H8n2C5F093PjzXZV8HCQX6z5olKBZRndUr5h7u8pim5qiTyh56rudUqcbrb0hHAI2Zv32JYuD67y/Puqz4MgJAMTlioSboQQIp/K2prxVW8/RjX3YkbgJf4ODmf9ydv8dTqcfvXdeaNNZZytpaPsM1OpwMJBt/x7PJ5sOcHn36Hnwdf370JihG759xNc2Syc8l7pyV5M5epbSSPhRgghnlIlJ0t+HFSHV2/F8c3Wi+y6FM2SQ6GsOX6LYY0r8GpzL+wsTAxdZsljbq9bytXL+7379x4Eneu5Q8/dq5B8B5KidEvogbz7WpTJfZXHroKuf49dBd3cW6W9A3kxJOFGCCGeUY1yNix+qQGHrt3hqy0XORZyj3m7rrH0YCijmlfkpaaeWGjln9lCYWaXt0NztvuxcC879FzPfdsrKfqf5eahvPuaWP7zKPx/g49teTAqAdN1lEDyt04IIZ5Tw4oOrH7Vn50Xo/hq80UuRCQwPfASi/bfYEyrSgxsWB5TjUxVYDBmtmBWWzfA4H+lxOe+ynPvBty9ofszPgzSEnWPuEeeybuvSg025XRh57/Bx95TNwK0MAgJN0IIoQcqlYrW3s60rOLEn6dvMyPwEjfuJDP5r3Ms3HON8W2r0LNOWYyN1IYuVfybqbVuJnS3Wnm/l54CcTd1V3vu3dBd/cn5+gZk3P9n4tHru/Lub2aXO+z8+2srN1DLZ6GgSLgRQgg9UqtVdK9Vls41XFl19BY/bL/M7bgU/rfmNHN3X+XtdlXpVN0FtVr6cRR5GlNwrKxb/ktRIDHyEcHnuu421/17uuX28bz7G5mArcfDg4+th4zn85wk3AghRAHQGKkZ2LA8PeuU5bcDIcwOusK16CTGLD1O9bLWTGhflRZVyshox8WVSgVWLrrFwz/v91MT/wk6927kDj6xoZCZBncu65aHsXTJ28/H3hMsy4KSVWBvq6SQcCOEEAXIVGPEy80r0r+BOwv3XGfhnmucCYtn+C9HaOBpz7sdq1LXw97QZQp901qCS3Xd8l+ZGbr+PA+74nP3hm4gw+zH2m8ezLWrBuiKEaorZXSTnFo6P1jK6P60ePCnpZNuMbUtlU97SbgRQohCYGWq4c12VRjq78GcoKv8ejCEw9fv0mvOAVp7OzGhfVWqucl4K6WCkTHYeegWWuT9fvLdh/fxuXcDJe4WajL/CT9PPJeJbowfy3+FHgun/wSiB0FIa1VigpCEGyGEKEQOllo+eqEaLzX15Iftl1l17BY7LkSx40IU3R6MduzpKKMdl2rZ4/mUrZPnWxkpyez4cwWtG1ZHk3JP1+8nKQoSo3RfJ0b/sy4lTnf7K3serycxNv1P+HH6Z/lvICriU11IuBFCCANwszXji141GdW8It8GXuKv0+FsOHWbv4PD6VvPnTfaVMLVRmbHFv9hpCHFxB5ca4HmCWPspKfoOjYnPhjA8N/hJzHywfcerEtLgIyUf57+ehKNxSPCz4PX9hV1s8YbiIQbIYQwoIplLJk1sA6vtohj+taL7LwYzbLDD0Y79vdgdMtK2Mtox+JZaEzB1l23PEla8r+uAEX9J/z8+8pQlO4R+PSkB52lrz/8eD5dod/v+n0/T0HCjRBCFAHVy9rwy4sNOHz9Ll9vucCRG/dYsOc6yw7fZGQzT0Y2q4iljHYsCoqJOZhU0D2Z9TiKohvYMDvwPCoQlTHcVRuQcCOEEEVKA097Vr7iT9ClaL7efJFz4fF8t+0yvx4I4bWWXgxu5CGjHQvDUal0HY+1VrqJSIsoGR5RCCGKGJVKRauqTvz1elNmDaxNRUcL7ial8dnf52n1TRDLD4eSkSljnQjxKAYNN9OmTaN+/fpYWVnh5OREQEAAFy9efOw+ixYtQqVS5VpMTU0LqWIhhCg8arWKF2q6sfXN5nzZqwauNqaEx6Xw3tpg2s3YzZ+nbpOVpRi6TCGKHIOGm127djFmzBgOHjxIYGAg6enptG/fnqSkpMfuZ21tTXh4eM4SEhJSSBULIUThMzZS069+eXZOaMlHXXywtzDhekwSry87wQsz97LzQhSKIiFHiGwG7XOzefPmXK8XLVqEk5MTx44do3nz5o/cT6VS4eLiUtDlCSFEkWKqMWJks4r0b1Cen/ZcZ8Gea5wLj+fFRUeoUdaGkc086VzDFY1MzilKuSLVoTguLg4Ae/vHD0WemJiIh4cHWVlZ1KlTh88//xxfX9+HbpuamkpqamrO6/j4eADS09NJT0/XU+XkHPPff4pnI+2oH9KO+lEU21GrhtdaVGBAfTfm7b7O74duEhwWx7jlJ/li0wWGNipPv3plsTJ9wjgohagotmNxVJrb8Wnes0opItcys7Ky6NatG7Gxsezdu/eR2x04cIDLly9Ts2ZN4uLi+Oabb9i9ezdnz56lXLlyebafOHEikyZNyrN+6dKlmJvLrKtCiOIvMR32RqjYE6kmMV03fL5WrdDIWaGFSxYO0i1RlADJyckMHDiQuLg4rK0fP1VJkQk3o0ePZtOmTezdu/ehIeVR0tPT8fHxYcCAAUyZMiXP9x925cbd3Z2YmJgnNs7TSk9PJzAwkHbt2qF50siR4pGkHfVD2lE/ilM7pqZnsuF0OD/vC+FKtK7voloFHao581ITD2q52xqstuLUjkVZaW7H+Ph4HB0d8xVuisRtqbFjx/LXX3+xe/fupwo2ABqNhtq1a3PlypWHfl+r1aLVah+6X0F9MAry2KWJtKN+SDvqR3FoR41Gw8BGngxoWIFdl6L5ae919lyOYdPZSDadjaSuhx0vN/OkXTUXjNSGmSCxOLRjcVAa2/Fp3q9Bw42iKLz++uusW7eOoKAgPD09n/oYmZmZBAcH07lz5wKoUAghih+VSkXLqk60rOrE+fB4Fu65zoZTYRwLucexkHuUtzfnpSYV6FPPHQsZ9ViUQAbtUj9mzBh+//13li5dipWVFREREURERHD//v2cbYYOHcr777+f83ry5Mls3bqVa9eucfz4cQYPHkxISAgjR440xFsQQogizcfVmul9/dj3bmvGtqqErbmG0LvJTPzzHP7TtvPFpgtExKUYukwh9MqgkX3OnDkAtGzZMtf6X375heHDhwMQGhqKWv1PBrt37x4vv/wyERER2NnZUbduXfbv30+1atUKq2whhCh2nKxNmdChKq+18mLN8TB+3nud6zFJzN11lYV7rtHVz40RTT2pXtbG0KUK8dwMflvqSYKCgnK9njFjBjNmzCigioQQomQzNzFmSCMPBjUoz7bzkSzce53D1++y7kQY606E4V/RgZHNPGlV1Qm1gfrlCPG85GarEEKUQmq1iva+LrT3deH0rVgW7rnO38HhHLh2hwPX7uBVxoIRTSvSs05ZmahTFDsyjKUQQpRyNcvZ8sOA2uz+XytGNa+IldaYq9FJfLAumMZf7ODbwEtEJ6Q++UBCFBESboQQQgBQ1taMDzr7sP/91nz8QjXK2ppxNymNH7ZfpsmXO3h39WkuRSYYukwhnkjCjRBCiFysTDWMaOrJrnda8uPAOtRytyUtI4sVR2/SfsZuhv18mD2Xo2WyTlFkSZ8bIYQQD2VspKZLTVc613DheOg9Fuy+zpZzEey6FM2uS9F4u1gxoqkn3Wq5oTWWfjmi6JArN0IIIR5LpVJR18OeuUPqEjShJcMbV8DcxIgLEQm8s/o0Tb/cyawdl7mXlGboUoUAJNwIIYR4Ch4OFkzs5suB99rwbkdvnK21RCek8s3WS/h/sZ2P15/hekySocsUpZyEGyGEEE/NxlzD6JZe7Plfa2b086OaqzUp6Vn8djCE1tODGLn4KIeu3ZF+OcIgpM+NEEKIZ2ZirKZH7XIE1CrLgWt3WLjnOjsuRLHtfCTbzkdSs5wNI5p60s7b0dClilJEwo0QQojnplKpaOzlSGMvR65EJfLT3uusPX6L07fiGLf8JK42ptS2VlE7LoXyjqVrNmtR+OS2lBBCCL2q5GTJtJ412P9ea95sWwVHSxPC41LYeNOIltN3M/yXw2wKDictI8vQpYoSSq7cCCGEKBAOllrGta3MKy0q8seJm8wPPMPVBBVBF6MJuhiNg4UJPWqXpV99dyo7Wxm6XFGCSLgRQghRoEw1RvSsXRbT8FP4NGjBulMRrD52i+iEVBbuvc7CvdepXd6WvvXc6ernhqVWfjWJ5yOfICGEEIXG09GCdzt683a7KgRdjGbF0ZvsuBDFidBYToTGMvnPc3Sp6Uq/+u7U87BDpZKZycXTk3AjhBCi0BkbqWlbzZm21ZyJSkhh3fEwVhy9ybXoJFYfu8XqY7eoWMaCvvXc6VmnLE5WpoYuWRQjEm6EEEIYlJOVKa+08GJU84ocC7nHiiM3+Ts4nGvRSXyx6QJfb7lIq6pO9KvvTquqZTA2kmdhxONJuBFCCFEkqFQq6lWwp14Fez7t5stfp26z8uhNjofG5oybU8ZKS6865ehbrxwVy1gaumRRREm4EUIIUeRYao3p36A8/RuU53JkAiuP3mTt8TCiE1KZu+sqc3ddpUEFe/rUK0eXmq6Ym8ivM/EP+TQIIYQo0io7W/Fhl2q808GbHRciWXHkJrsuRXP4xl0O37jLpD/P0dXPlb713KnlbiudkIWEGyGEEMWDibGajtVd6VjdlYi4FNYcv8XKozcJuZPMssM3WXb4JlWcLelbz50etcviYKk1dMnCQCTcCCGEKHZcbEwZ06oSo1t4cej6XVYevcnG4HAuRSby2d/n+XLzBdr6ONO3vjvNK5fBSC1Xc0oTCTdCCCGKLbVahb+XA/5eDkzs5suGU7dZdfQmp2/FselMBJvOROBqY0rvuuXoU9ed8g7mhi5ZFAIJN0IIIUoEGzMNQxp5MKSRB+dux7Py6E3WnwwjPC6FmTuuMHPHFRp7OdC3njsdq7tgqjEydMmigEi4EUIIUeJUc7NmYjdf3uvkTeC5SFYevcneKzHsv3qH/VfvYP2HMd1r6ea1ql7WxtDlCj2TcCOEEKLEMtUY0dXPja5+bty6l8zqY7dYdfQWYbH3+e1gCL8dDKGaqzX96rsTUKssNuYaQ5cs9EDCjRBCiFKhnJ0549tW4Y3Wldl3NYYVR26y9Wwk58Lj+XTDWaZuPE8bbye6+bnRyttJblsVYxJuhBBClCpqtYpmlcvQrHIZ7iWlsf5kGCuO3ORCREJOJ2RLrTHtqznT1c+NppUd0ciUD8WKhBshhBCllp2FCS828WR44wqcvR3Pn6dv89epcMJi77P2RBhrT4Rha66hU3VXuvm50cDTXh4rLwYk3AghhCj1VCoV1cvaUL2sDe928ObEzXtsOHmbv4PDiUlMY9nhUJYdDsXZWkuXGm509XOV0ZCLMAk3QgghxL+o1SrqethT18Oej1+oxsFrd/nz1G02nQknMj6Vn/dd5+d913G3N6NrTTe61XLD28Xa0GWLf5FwI4QQQjyCsZGappUdaVrZkckBvuy5FMOGU7cJPBfJzbv3mR10ldlBV6nibEnXmrqnsio4Whi67FJPwo0QQgiRD1pjI9pWc6ZtNWeS0zLYfj6KDadus+tiNJciE5keeInpgZeoWc6Gbn5udKnpiquNmaHLLpUk3AghhBBPydzEOGf8nLj76Ww5G8Gfp26z/+odTt+K4/StOKZuPE/9CvZ09XOjc3UXmcizEEm4EUIIIZ6DjZmGvvXc6VvPnZjEVDYFh7Ph1G2O3LjH4et3OXz9LhM3nKVJJUe6+bnR3tcZa1MZLLAgSbgRQggh9MTRUssQ/woM8a/A7dj7/HX6Nn+eCic4LI7dl6LZfSkak3VqWlUtQ1c/N9p4O2NmIoMF6puEGyGEEKIAuNmaMaq5F6Oae3EtOpG/Tuuu6FyJSmTL2Ui2nI3E3MSIdtWc6ebnRrPKZTAxlsEC9cGgrTht2jTq16+PlZUVTk5OBAQEcPHixSfut2rVKry9vTE1NaVGjRps3LixEKoVQgghnk3FMpa80aYygW82Z9O4Zoxu6UU5OzOS0zL54+RtRiw+Sv2p23hvzWn2XYkhM0sxdMnFmkHDza5duxgzZgwHDx4kMDCQ9PR02rdvT1JS0iP32b9/PwMGDGDEiBGcOHGCgIAAAgICOHPmTCFWLoQQQjw9lUqFj6s173b0Zs//WrH2tca82KQCZay0xN1PZ/mRmwxaeIiGn29n4oazHAu5h6JI0HlaBr0ttXnz5lyvFy1ahJOTE8eOHaN58+YP3ef777+nY8eOvPPOOwBMmTKFwMBAZs2axdy5cwu8ZiGEEEIfVCoVdcrbUae8HR91qcaha3f48/RtNgZHEJOYyqL9N1i0/wZlbc0ePJnlSmVHebQ8P4pUn5u4uDgA7O3tH7nNgQMHeOutt3Kt69ChA+vXr3/o9qmpqaSmpua8jo+PByA9PZ309PTnrDi37OPp+7iljbSjfkg76oe0o35IOz5ZfQ8b6nvY8FGnquy7eoe/Tkew7UIUYbH3mbvrKnN3XaWiozleWjXON+5Qu7w96lI0z9XTfHZUShG53pWVlUW3bt2IjY1l7969j9zOxMSExYsXM2DAgJx1s2fPZtKkSURGRubZfuLEiUyaNCnP+qVLl2Jubq6f4oUQQogCkJYJZ2NVHI9Rce6eigzlnzBjo1HwtVeoYadQxUahpPdFTk5OZuDAgcTFxWFt/fjpLorMlZsxY8Zw5syZxwabZ/H+++/nutITHx+Pu7s77du3f2LjPK309HQCAwNp164dGo2MYfCspB31Q9pRP6Qd9UPa8dkFPPgzISWdrWcjWL77LJcSNcSlZbI/UsX+SLAwMaJpJQfa+jjRskoZbM1LXhtn33nJjyIRbsaOHctff/3F7t27KVeu3GO3dXFxyXOFJjIyEhcXl4dur9Vq0Wrzjgqp0WgK7C9YQR67NJF21A9pR/2QdtQPacdnZ6/R0KuuO2aRwbRp15IjN+MJPBfJtnORRCWksuVcFFvORWGkVlG/gh3tqrnQvpoz7vYl4y7F03xuDBpuFEXh9ddfZ926dQQFBeHp6fnEffz9/dm+fTvjx4/PWRcYGIi/v38BViqEEEIUHVqNEa2qOtGqqhOfda9OcFgcgeciCTwXycXIBA5eu8vBa3eZ8tc5qjpb0a6aM+2qOVOjrE2p6Kdj0HAzZswYli5dyh9//IGVlRUREREA2NjYYGam6xE+dOhQypYty7Rp0wAYN24cLVq0YPr06XTp0oXly5dz9OhR5s+fb7D3IYQQQhiKWq3Cz90WP3dbJnSoSuidZALPRxJ4LoIjN+5xMTKBi5EJzNp5BWdrLW19dJN/NvZyQGtcMkdHNmi4mTNnDgAtW7bMtf6XX35h+PDhAISGhqJW/9NLqnHjxixdupSPPvqIDz74gMqVK7N+/XqqV69eWGULIYQQRVZ5B3NGNPVkRFNPYpPT2HEhim3nI9l1MZrI+FSWHAplyaFQLEyMaFG1DO2qOdOqqhO25iaGLl1vDH5b6kmCgoLyrOvTpw99+vQpgIqEEEKIksPW3ISedcrRs045UtIzOXDtTq5+OhuDI9gYHIGRWkWDCva0reZcIvrpFIkOxUIIIYQoWKZP6Kdz4NodDly7w5S/zuHtkrufjkpVvPrpSLgRQgghSpkn9dO5EJHAhYgEZu74p59Ou2rO+BeTfjoSboQQQohSLr/9dCy1xrSoUoa21ZxoXdUZmyI6no6EGyGEEELkeFI/nb+Dw/k7ODynn0727aui1E9Hwo0QQgghHiq//XQmF7F+OhJuhBBCCPFED+uns/VcBNvOR+bpp9O0kiO/j2xosFol3AghhBDiqZV3MGdks4qMbFaRe0lp7LwYReC5SHZdisbP3cagtUm4EUIIIcRzsbPI3U8nNSPLoPVIuBFCCCGE3phqjDDVGPZxcfWTNxFCCCGEKD4k3AghhBCiRJFwI4QQQogSRcKNEEIIIUoUCTdCCCGEKFEk3AghhBCiRJFwI4QQQogSRcKNEEIIIUoUCTdCCCGEKFEk3AghhBCiRJFwI4QQQogSRcKNEEIIIUoUCTdCCCGEKFFK3azgiqIAEB8fr/djp6enk5ycTHx8PBqNRu/HLy2kHfVD2lE/pB31Q9pRP0pzO2b/3s7+Pf44pS7cJCQkAODu7m7gSoQQQgjxtBISErCxsXnsNiolPxGoBMnKyuL27dtYWVmhUqn0euz4+Hjc3d25efMm1tbWej12aSLtqB/Sjvoh7agf0o76UZrbUVEUEhIScHNzQ61+fK+aUnflRq1WU65cuQI9h7W1dan70BUEaUf9kHbUD2lH/ZB21I/S2o5PumKTTToUCyGEEKJEkXAjhBBCiBJFwo0eabVaPv30U7RaraFLKdakHfVD2lE/pB31Q9pRP6Qd86fUdSgWQgghRMkmV26EEEIIUaJIuBFCCCFEiSLhRgghhBAlioQbIYQQQpQoEm705Mcff6RChQqYmprSsGFDDh8+bOiSipVp06ZRv359rKyscHJyIiAggIsXLxq6rGLviy++QKVSMX78eEOXUuyEhYUxePBgHBwcMDMzo0aNGhw9etTQZRUrmZmZfPzxx3h6emJmZoaXlxdTpkzJ19xApdnu3bvp2rUrbm5uqFQq1q9fn+v7iqLwySef4OrqipmZGW3btuXy5cuGKbaIknCjBytWrOCtt97i008/5fjx4/j5+dGhQweioqIMXVqxsWvXLsaMGcPBgwcJDAwkPT2d9u3bk5SUZOjSiq0jR44wb948atasaehSip179+7RpEkTNBoNmzZt4ty5c0yfPh07OztDl1asfPnll8yZM4dZs2Zx/vx5vvzyS7766itmzpxp6NKKtKSkJPz8/Pjxxx8f+v2vvvqKH374gblz53Lo0CEsLCzo0KEDKSkphVxpEaaI59agQQNlzJgxOa8zMzMVNzc3Zdq0aQasqniLiopSAGXXrl2GLqVYSkhIUCpXrqwEBgYqLVq0UMaNG2fokoqVd999V2natKmhyyj2unTporz00ku51vXs2VMZNGiQgSoqfgBl3bp1Oa+zsrIUFxcX5euvv85ZFxsbq2i1WmXZsmUGqLBokis3zyktLY1jx47Rtm3bnHVqtZq2bdty4MABA1ZWvMXFxQFgb29v4EqKpzFjxtClS5dcn0uRfxs2bKBevXr06dMHJycnateuzYIFCwxdVrHTuHFjtm/fzqVLlwA4deoUe/fupVOnTgaurPi6fv06ERERuf5u29jY0LBhQ/md8y+lbuJMfYuJiSEzMxNnZ+dc652dnblw4YKBqiresrKyGD9+PE2aNKF69eqGLqfYWb58OcePH+fIkSOGLqXYunbtGnPmzOGtt97igw8+4MiRI7zxxhuYmJgwbNgwQ5dXbLz33nvEx8fj7e2NkZERmZmZTJ06lUGDBhm6tGIrIiIC4KG/c7K/JyTciCJozJgxnDlzhr179xq6lGLn5s2bjBs3jsDAQExNTQ1dTrGVlZVFvXr1+PzzzwGoXbs2Z86cYe7cuRJunsLKlStZsmQJS5cuxdfXl5MnTzJ+/Hjc3NykHUWBkttSz8nR0REjIyMiIyNzrY+MjMTFxcVAVRVfY8eO5a+//mLnzp2UK1fO0OUUO8eOHSMqKoo6depgbGyMsbExu3bt4ocffsDY2JjMzExDl1gsuLq6Uq1atVzrfHx8CA0NNVBFxdM777zDe++9R//+/alRowZDhgzhzTffZNq0aYYurdjK/r0iv3MeT8LNczIxMaFu3bps3749Z11WVhbbt2/H39/fgJUVL4qiMHbsWNatW8eOHTvw9PQ0dEnFUps2bQgODubkyZM5S7169Rg0aBAnT57EyMjI0CUWC02aNMkzFMGlS5fw8PAwUEXFU3JyMmp17l8zRkZGZGVlGaii4s/T0xMXF5dcv3Pi4+M5dOiQ/M75F7ktpQdvvfUWw4YNo169ejRo0IDvvvuOpKQkXnzxRUOXVmyMGTOGpUuX8scff2BlZZVz79jGxgYzMzMDV1d8WFlZ5emnZGFhgYODg/RfegpvvvkmjRs35vPPP6dv374cPnyY+fPnM3/+fEOXVqx07dqVqVOnUr58eXx9fTlx4gTffvstL730kqFLK9ISExO5cuVKzuvr169z8uRJ7O3tKV++POPHj+ezzz6jcuXKeHp68vHHH+Pm5kZAQIDhii5qDP24Vkkxc+ZMpXz58oqJiYnSoEED5eDBg4YuqVgBHrr88ssvhi6t2JNHwZ/Nn3/+qVSvXl3RarWKt7e3Mn/+fEOXVOzEx8cr48aNU8qXL6+YmpoqFStWVD788EMlNTXV0KUVaTt37nzov4fDhg1TFEX3OPjHH3+sODs7K1qtVmnTpo1y8eJFwxZdxKgURYaKFEIIIUTJIX1uhBBCCFGiSLgRQgghRIki4UYIIYQQJYqEGyGEEEKUKBJuhBBCCFGiSLgRQgghRIki4UYIIYQQJYqEGyFEqadSqVi/fr2hyxBC6ImEGyGEQQ0fPhyVSpVn6dixo6FLE0IUUzK3lBDC4Dp27Mgvv/ySa51WqzVQNUKI4k6u3AghDE6r1eLi4pJrsbOzA3S3jObMmUOnTp0wMzOjYsWKrF69Otf+wcHBtG7dGjMzMxwcHBg1ahSJiYm5tvn555/x9fVFq9Xi6urK2LFjc30/JiaGHj16YG5uTuXKldmwYUPBvmkhRIGRcCOEKPI+/vhjevXqxalTpxg0aBD9+/fn/PnzACQlJdGhQwfs7Ow4cuQIq1atYtu2bbnCy5w5cxgzZgyjRo0iODiYDRs2UKlSpVznmDRpEn379uX06dN07tyZQYMGcffu3UJ9n0IIPTH0zJ1CiNJt2LBhipGRkWJhYZFrmTp1qqIouhnjX3311Vz7NGzYUBk9erSiKIoyf/58xc7OTklMTMz5/t9//62o1WolIiJCURRFcXNzUz788MNH1gAoH330Uc7rxMREBVA2bdqkt/cphCg80udGCGFwrVq1Ys6cObnW2dvb53zt7++f63v+/v6cPHkSgPPnz+Pn54eFhUXO95s0aUJWVhYXL15EpVJx+/Zt2rRp89gaatasmfO1hYUF1tbWREVFPetbEkIYkIQbIYTBWVhY5LlNpC9mZmb52k6j0eR6rVKpyMrKKoiShBAFTPrcCCGKvIMHD+Z57ePjA4CPjw+nTp0iKSkp5/v79u1DrVZTtWpVrKysqFChAtu3by/UmoUQhiNXboQQBpeamkpERESudcbGxjg6OgKwatUq6tWrR9OmTVmyZAmHDx/mp59+AmDQoEF8+umnDBs2jIkTJxIdHc3rr7/OkCFDcHZ2BmDixIm8+uqrODk50alTJxISEti3bx+vv/564b5RIUShkHAjhDC4zZs34+rqmmtd1apVuXDhAqB7kmn58uW89tpruLq6smzZMqpVqwaAubk5W7ZsYdy4cdSvXx9zc3N69erFt99+m3OsYcOGkZKSwowZM5gwYQKOjo707t278N6gEKJQqRRFUQxdhBBCPIpKpWLdunUEBAQYuhQhRDEhfW6EEEIIUaJIuBFCCCFEiSJ9boQQRZrcORdCPC25ciOEEEKIEkXCjRBCCCFKFAk3QgghhChRJNwIIYQQokSRcCOEEEKIEkXCjRBCCCFKFAk3QgghhChRJNwIIYQQokSRcCOEEEKIEuX/tcg1hgmu+pMAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Helper - loss visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(train_losses, dev_losses):\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(dev_losses, label='Dev Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Dev Loss over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_losses(train_losses, dev_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "093085ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "093085ae",
        "outputId": "e496f8b6-851f-4327-ed42-56ef700d08bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Exactly match accuracy - Dev set   : 2.09%\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# added value - accuracy\n",
        "def compute_exact_accuracy(pred_sentences, target_sentences):\n",
        "    correct = 0\n",
        "    for pred, target in zip(pred_sentences, target_sentences):\n",
        "        if pred == target:\n",
        "            correct += 1\n",
        "    return correct / len(pred_sentences)\n",
        "\n",
        "# evaluate exact match on dev set after training\n",
        "model.eval()\n",
        "pred_sentences = []\n",
        "target_sentences = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, trg in dev_loader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        output = model(src, trg)  # shape: (batch, seq_len, vocab_size)\n",
        "        output_indices = output.argmax(dim=2)  # shape: (batch, seq_len)\n",
        "\n",
        "        # convert predictions and targets to text\n",
        "        for out_seq, tgt_seq in zip(output_indices.cpu().tolist(), trg.cpu().tolist()):\n",
        "            out_words = [idx2word[idx] for idx in out_seq if idx2word[idx] not in [PAD, EOS]]\n",
        "            tgt_words = [idx2word[idx] for idx in tgt_seq if idx2word[idx] not in [PAD, EOS]]\n",
        "            pred_sentences.append(\" \".join(out_words))\n",
        "            target_sentences.append(\" \".join(tgt_words))\n",
        "\n",
        "# calculate and print accuracy\n",
        "exact_match_acc = compute_exact_accuracy(pred_sentences, target_sentences)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Exactly match accuracy - Dev set':<35}: {exact_match_acc * 100:.2f}%\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d7489094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7489094",
        "outputId": "d3ec90f7-0dcc-4ee6-9ec4-34ef594a79bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "sample predictions on Dev Set:\n",
            "============================================================\n",
            "Target   : that is why he wasn't present at the meeting.\n",
            "Predicted: he she not he is at at the moment.\n",
            "------------------------------------------------------------\n",
            "Target   : i don't like to mix business with pleasure.\n",
            "Predicted: i don't like to speak to to us. saw. saw. saw. saw. bill.\n",
            "------------------------------------------------------------\n",
            "Target   : tom didn't ask for an apology.\n",
            "Predicted: tom didn't ask mary an accident.\n",
            "------------------------------------------------------------\n",
            "Target   : is there a supermarket near here?\n",
            "Predicted: is there a new in here?\n",
            "------------------------------------------------------------\n",
            "Target   : tom plays golf every monday.\n",
            "Predicted: tom left golf every weekend.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"sample predictions on Dev Set:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "samples = random.sample(range(len(pred_sentences)), 5)\n",
        "\n",
        "for i in samples:\n",
        "    print(f\"Target   : {target_sentences[i]}\")\n",
        "    print(f\"Predicted: {pred_sentences[i]}\")\n",
        "    print(\"-\" * 60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aO0t5R_HkEvF",
      "metadata": {
        "id": "aO0t5R_HkEvF"
      },
      "source": [
        "### Pedagogical Note: Interpreting the Loss Plots ğŸ”\n",
        "\n",
        "After training completes, you'll see two curves:\n",
        "\n",
        "- **Train Loss**: How well the model fits the training data.\n",
        "- **Dev Loss**: How well the model generalizes to unseen (validation) data.\n",
        "\n",
        "Ideally:\n",
        "- Both curves should **decrease over time**.\n",
        "- A small gap between them indicates **good generalization**.\n",
        "- If train loss decreases but dev loss increases, your model may be **overfitting**.\n",
        "\n",
        "Use these curves to decide if your model is learning stably and when to stop training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fSXqwoPDndhF",
      "metadata": {
        "id": "fSXqwoPDndhF"
      },
      "source": [
        "## Step 4: Inference\n",
        "Now that you have a trained model, you can use it to solve your task on any new data points.\n",
        "\n",
        "Complete the code below for implementing a inference function for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "LOlykSQOoa_R",
      "metadata": {
        "id": "LOlykSQOoa_R"
      },
      "outputs": [],
      "source": [
        "def infer_sentences(shuffled_sentences, model, word2idx, idx2word, device, max_len=16):\n",
        "    \"\"\"\n",
        "    Generate predicted (unshuffled) sentences from a list of shuffled input sentences.\n",
        "\n",
        "    Args:\n",
        "        shuffled_sentences (List[str]): Sentences with words in randomized order.\n",
        "        model (Seq2Seq): Trained encoder-decoder model.\n",
        "        word2idx (Dict[str, int]): Mapping from words to vocabulary indices.\n",
        "        idx2word (Dict[int, str]): Mapping from indices to words.\n",
        "        device (torch.device): CPU or CUDA device for inference.\n",
        "        max_len (int): Maximum sentence length to decode.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Model's predicted sentences (as plain strings).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    sos_idx = word2idx['<SOS>']\n",
        "    eos_idx = word2idx['<EOS>']\n",
        "    pad_idx = word2idx['<PAD>']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentence in shuffled_sentences:\n",
        "            # TODO\n",
        "            predicted_sentence: str = ...\n",
        "            predictions.append(predicted_sentence)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ddv1mOKWqA9p",
      "metadata": {
        "id": "Ddv1mOKWqA9p"
      },
      "source": [
        "\n",
        "Now, use the above inference function to generate prediction for the **test set**. You are provided with the test set shuffled sentences in [this downloadable file](https://drive.google.com/file/d/178mEesTW89Ooz_f5bb6sHBHjAkG4DWMV/view?usp=sharing). In your submission, you should run the inference function of the provided list of shuffled sentences, to attain a `predicted_test.csv` file. Typically, these predictions over the test set will be evaluated against their annotated labels (targets).  \n",
        "\n",
        "If you want to assess the quality of current trained model, you can run inference on the dev set and evaluate on it using the function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "SE0shbcytAE1",
      "metadata": {
        "id": "SE0shbcytAE1"
      },
      "outputs": [],
      "source": [
        "def evaluate_sentence_predictions(predictions, targets):\n",
        "    \"\"\"\n",
        "    Compute exact match accuracy between predicted and target sentences (as strings).\n",
        "    Args:\n",
        "        predictions (List[str])\n",
        "        targets (List[str])\n",
        "    Returns:\n",
        "        accuracy (float): percentage of exact string matches\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = len(predictions)\n",
        "    for pred, true in zip(predictions, targets):\n",
        "        if pred.strip().lower == true.strip().lower:\n",
        "            correct += 1\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Exact match accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m9PRjqHkt50Z",
      "metadata": {
        "id": "m9PRjqHkt50Z"
      },
      "source": [
        "# ğŸ” Stage 2: Add Attention to Your Seq2Seq Model\n",
        "\n",
        "In this stage, you'll implement a new model: **`Seq2SeqWithAttention`**.  \n",
        "The goal is to improve your decoder by allowing it to \"attend\" to relevant parts of the encoder's outputs at each decoding step.\n",
        "\n",
        "### ğŸ§  Why Attention?\n",
        "The encoder compresses the entire input sentence into a single hidden state. This limits performance, especially on long sentences.\n",
        "\n",
        "With attention, the decoder **dynamically focuses on different encoder outputs**, depending on what it's trying to generate.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”§ What You Need to Implement:\n",
        "You will create a new class `Seq2SeqWithAttention`, similar to `Seq2Seq`, but with the following differences:\n",
        "\n",
        "1. At each decoding step, compute **attention scores** over all encoder hidden states.\n",
        "2. Use those scores to compute a **context vector** (weighted sum of encoder outputs).\n",
        "3. Feed the context vector into the decoder along with the embedding of the current input token.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Œ Tip: Dot-Product Attention\n",
        "A simple form of attention you can implement is dot-product attention:\n",
        "\n",
        "```python\n",
        "score_t = dot(h_dec_t, h_enc_i) weights = softmax(score_t over i) context_t = sum_i weights[i] * h_enc_i\n",
        "```\n",
        "\n",
        "You'll apply this at every decoder time step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "nPvAmc_euarf",
      "metadata": {
        "id": "nPvAmc_euarf"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder, sos_idx, device):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sos_idx = sos_idx\n",
        "        self.device = device\n",
        "\n",
        "        # TODO: If needed, add additional layers for attention (e.g., linear projection)\n",
        "\n",
        "    def compute_attention(self, decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        decoder_hidden: (1, batch_size, hidden_size)\n",
        "        encoder_outputs: (seq_len, batch_size, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context vector: (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        # TODO: implement dot-product attention or another variant\n",
        "        # Steps:\n",
        "        # 1. Compute attention scores: dot(decoder_hidden, encoder_outputs)\n",
        "        # 2. Apply softmax to get attention weights\n",
        "        # 3. Compute context vector: weighted sum of encoder_outputs\n",
        "        return context_vector\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        src: (batch_size, seq_len)\n",
        "        trg: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            outputs: (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, trg_len = trg.shape\n",
        "        vocab_size = self.decoder.out.out_features\n",
        "        outputs = torch.zeros(batch_size, trg_len, vocab_size, device=self.device)\n",
        "\n",
        "        src = src.T\n",
        "        trg = trg.T\n",
        "\n",
        "        hidden = self.encoder.init_hidden(batch_size)\n",
        "        encoder_outputs, hidden = self.encoder(src, hidden)  # encoder_outputs: (seq_len, batch_size, hidden_size)\n",
        "\n",
        "        input_token = torch.full((batch_size,), self.sos_idx, device=self.device)\n",
        "\n",
        "        for t in range(trg_len):\n",
        "            # TODO: Compute context vector based on current hidden and encoder_outputs\n",
        "            context = self.compute_attention(hidden, encoder_outputs)\n",
        "\n",
        "            # TODO: Modify decoder forward to accept and use the context vector\n",
        "            # Hint: You may need to add context to the input embedding, or to the hidden state\n",
        "            output, hidden = self.decoder(input_token, hidden, context)\n",
        "\n",
        "            outputs[:, t, :] = output\n",
        "            input_token = trg[t]  # teacher forcing\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "01e6cc6c",
      "metadata": {
        "id": "01e6cc6c"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder, sos_idx, device):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sos_idx = sos_idx\n",
        "        self.device = device\n",
        "\n",
        "        # TODO: If needed, add additional layers for attention (e.g., linear projection)\n",
        "        # No extra layers needed for basic dot-product attention\n",
        "\n",
        "    def compute_attention(self, decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        decoder_hidden: (h_n, c_n), each of shape (1, batch_size, hidden_size)\n",
        "        encoder_outputs: (seq_len, batch_size, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context vector: (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        # TODO: implement dot-product attention or another variant\n",
        "        # Steps:\n",
        "        # 1. Compute attention scores: dot(decoder_hidden, encoder_outputs)\n",
        "        # 2. Apply softmax to get attention weights\n",
        "        # 3. Compute context vector: weighted sum of encoder_outputs\n",
        "\n",
        "        h = decoder_hidden[0]  # Take hidden state h_n from (h_n, c_n)\n",
        "        h = h.transpose(0, 1)  # Shape: (batch_size, 1, hidden_size)\n",
        "\n",
        "        attn_scores = torch.bmm(\n",
        "            h,                                  # (batch_size, 1, hidden_size)\n",
        "            encoder_outputs.permute(1, 2, 0)    # (batch_size, hidden_size, seq_len)\n",
        "        )  # => (batch_size, 1, seq_len)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)  # (batch_size, 1, seq_len)\n",
        "        context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1))  # (batch_size, 1, hidden_size)\n",
        "\n",
        "        return context.squeeze(1)  # => (batch_size, hidden_size)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        src: (batch_size, seq_len)\n",
        "        trg: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            outputs: (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, trg_len = trg.shape\n",
        "        vocab_size = self.decoder.out.out_features\n",
        "        outputs = torch.zeros(batch_size, trg_len, vocab_size, device=self.device)\n",
        "\n",
        "        src = src.T\n",
        "        trg = trg.T\n",
        "\n",
        "        hidden = self.encoder.init_hidden(batch_size)\n",
        "        encoder_outputs, hidden = self.encoder(src, hidden)  # encoder_outputs: (seq_len, batch_size, hidden_size)\n",
        "\n",
        "        input_token = torch.full((batch_size,), self.sos_idx, device=self.device)\n",
        "\n",
        "        for t in range(trg_len):\n",
        "            # TODO: Compute context vector based on current hidden and encoder_outputs\n",
        "            context = self.compute_attention(hidden, encoder_outputs)\n",
        "\n",
        "            # TODO: Modify decoder forward to accept and use the context vector\n",
        "            # Hint: You may need to add context to the input embedding, or to the hidden state\n",
        "            output, hidden = self.decoder(input_token, hidden, context)\n",
        "\n",
        "            outputs[:, t, :] = output\n",
        "            input_token = trg[t]  # teacher forcing\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Fm5tMpGvBP1",
      "metadata": {
        "id": "3Fm5tMpGvBP1"
      },
      "source": [
        "## ğŸ“ What You Need to Complete\n",
        "\n",
        "- Implement the `compute_attention()` method to compute a context vector from encoder outputs given the current (decoder) hidden state.\n",
        "- Update your `DecoderRNN` class so that it accepts and uses a context vector at every step:\n",
        "    - One option is to **concatenate** the context vector with the embedded input token before passing it to the RNN.\n",
        "    - Alternatively, you can feed the context into a projection layer together with the decoder hidden state.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Œ Tip: Updated Decoder Signature\n",
        "\n",
        "You may need to rewrite your decoder to look like:\n",
        "\n",
        "```python\n",
        "def forward(self, input_token, hidden, context_vector):\n",
        "    ...\n",
        "```\n",
        "---\n",
        "> ğŸ’¡ **Note on Attention Implementation**\n",
        "\n",
        "You are given flexibility in how you choose to implement the attention mechanism.\n",
        "\n",
        "You may:\n",
        "- Implement the **basic dot-product attention** we discussed in class.\n",
        "- Or, explore a more expressive variant by adding a **linear projection** to the decoder hidden state or the encoder outputs â€” borrowing an idea from self-attention in Transformers.\n",
        "\n",
        "As long as your implementation uses the decoder hidden state to compute attention over the encoder outputs and forms a context vector, you're free to experiment and design the solution that makes most sense to you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lsGSMPTxwj4a",
      "metadata": {
        "id": "lsGSMPTxwj4a"
      },
      "source": [
        "## 2.2 - Evaluate and Compare\n",
        "\n",
        "In this stage, you should use your adapted `Seq2SeqWithAttention` model, train it and evaluate its performance (on the dev set).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "RRjxs544y1NK",
      "metadata": {
        "id": "RRjxs544y1NK"
      },
      "outputs": [],
      "source": [
        "# Instantiate and Train a Seq2SeqWithAttention model\n",
        "# TODO\n",
        "# decoder that supports attention: takes context vector as additional input\n",
        "class DecoderRNNWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(DecoderRNNWithAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size + hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, context):\n",
        "        \"\"\"\n",
        "        input: (batch_size)\n",
        "        hidden: (h_0, c_0)\n",
        "        context: (batch_size, hidden_size)\n",
        "        Returns: output logits (batch_size, vocab_size), and updated hidden state\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input).unsqueeze(0)  # (1, batch, embedding)\n",
        "        context = context.unsqueeze(0)                 # (1, batch, hidden)\n",
        "        lstm_input = torch.cat([embedded, context], dim=2)  # (1, batch, embedding + hidden)\n",
        "        output, hidden = self.lstm(lstm_input, hidden)\n",
        "        output = self.out(output.squeeze(0))           # (batch, vocab_size)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d8deb8f4",
      "metadata": {
        "id": "d8deb8f4"
      },
      "outputs": [],
      "source": [
        "# build encoder and decoder for attention model\n",
        "attention_encoder = EncoderRNN(vocab_size, embedding_size, hidden_size)\n",
        "attention_decoder = DecoderRNNWithAttention(vocab_size, embedding_size, hidden_size)\n",
        "\n",
        "# build Seq2SeqithAttention model\n",
        "attention_model = Seq2SeqWithAttention(attention_encoder, attention_decoder, word2idx[SOS], device)\n",
        "\n",
        "# set an optimizer and loss\n",
        "attention_optimizer = optim.Adam(attention_model.parameters(), lr=0.001)\n",
        "attention_criterion = nn.CrossEntropyLoss(ignore_index=word2idx[PAD])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7f474ec1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f474ec1",
        "outputId": "cb6da220-1c5f-4f7e-9e2c-4b5ed6434bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH       : 1 / 12\n",
            "Train Loss  : 5.1792\n",
            "Dev Loss    : 4.3954\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 2 / 12\n",
            "Train Loss  : 4.1701\n",
            "Dev Loss    : 3.7627\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 3 / 12\n",
            "Train Loss  : 3.6095\n",
            "Dev Loss    : 3.3884\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 4 / 12\n",
            "Train Loss  : 3.1930\n",
            "Dev Loss    : 3.0809\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 5 / 12\n",
            "Train Loss  : 2.8550\n",
            "Dev Loss    : 2.8488\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 6 / 12\n",
            "Train Loss  : 2.5652\n",
            "Dev Loss    : 2.6824\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 7 / 12\n",
            "Train Loss  : 2.3113\n",
            "Dev Loss    : 2.5591\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 8 / 12\n",
            "Train Loss  : 2.0842\n",
            "Dev Loss    : 2.4409\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 9 / 12\n",
            "Train Loss  : 1.8797\n",
            "Dev Loss    : 2.3529\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 10 / 12\n",
            "Train Loss  : 1.6977\n",
            "Dev Loss    : 2.2761\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 11 / 12\n",
            "Train Loss  : 1.5350\n",
            "Dev Loss    : 2.2156\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "EPOCH       : 12 / 12\n",
            "Train Loss  : 1.3886\n",
            "Dev Loss    : 2.1782\n",
            "============================================================\n",
            "\n",
            "training completed for Seq2SeqWithAttention model\n"
          ]
        }
      ],
      "source": [
        "# train the attention-based model\n",
        "attention_train_losses, attention_dev_losses = train_model(\n",
        "    attention_model, train_loader, dev_loader,\n",
        "    attention_optimizer, attention_criterion, device,\n",
        "    num_epochs=12\n",
        ")\n",
        "\n",
        "print(\"training completed for Seq2SeqWithAttention model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad7a9ed",
      "metadata": {
        "id": "dad7a9ed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "35faf856",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35faf856",
        "outputId": "408bd331-34d7-4055-9009-f74b1e05314d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Exactly match accuracy - Dev set   : 8.43%\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# added value - accuracy - this time with the attention based model\n",
        "def compute_exact_accuracy(pred_sentences, target_sentences):\n",
        "    correct = 0\n",
        "    for pred, target in zip(pred_sentences, target_sentences):\n",
        "        if pred == target:\n",
        "            correct += 1\n",
        "    return correct / len(pred_sentences)\n",
        "\n",
        "# evaluate exact match on dev set after training\n",
        "attention_model.eval()\n",
        "pred_sentences = []\n",
        "target_sentences = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, trg in dev_loader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        output = attention_model(src, trg)  # shape: (batch, seq_len, vocab_size)\n",
        "        output_indices = output.argmax(dim=2)  # shape: (batch, seq_len)\n",
        "\n",
        "        # convert predictions and targets to text\n",
        "        for out_seq, tgt_seq in zip(output_indices.cpu().tolist(), trg.cpu().tolist()):\n",
        "            out_words = [idx2word[idx] for idx in out_seq if idx2word[idx] not in [PAD, EOS]]\n",
        "            tgt_words = [idx2word[idx] for idx in tgt_seq if idx2word[idx] not in [PAD, EOS]]\n",
        "            pred_sentences.append(\" \".join(out_words))\n",
        "            target_sentences.append(\" \".join(tgt_words))\n",
        "\n",
        "# calculate and print accuracy\n",
        "exact_match_acc = compute_exact_accuracy(pred_sentences, target_sentences)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Exactly match accuracy - Dev set':<35}: {exact_match_acc * 100:.2f}%\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54e29c4",
      "metadata": {
        "id": "b54e29c4"
      },
      "source": [
        "## ğŸ“Œ Submission Instructions:\n",
        "Submit your completed notebook along with two files of predictions over the test set - `seq2seq_predictions.csv` and `seq2seq_with_attention_predictions.csv`.\n",
        "\n",
        "I will run the evaluation over the held-out test set.\n",
        "\n",
        "You should email all of these files with an email specifying the names and ids of the team (couples).\n",
        "\n",
        "âœ… **Good luck!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e23b36",
      "metadata": {
        "id": "11e23b36"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "UYc5jImTAg6l",
        "outputId": "e122df7c-0eb6-4d99-8fb4-161121e2eaa7"
      },
      "id": "UYc5jImTAg6l",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-521a1e68-0516-4303-9efe-ed4be68286f3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-521a1e68-0516-4303-9efe-ed4be68286f3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_no_target.csv to test_no_target (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_sentences(shuffled_sentences, model, word2idx, idx2word, device, max_len=16):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    sos_idx = word2idx['<SOS>']\n",
        "    eos_idx = word2idx['<EOS>']\n",
        "    pad_idx = word2idx['<PAD>']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentence in shuffled_sentences:\n",
        "            # tokenize and encode sentence\n",
        "            token_ids = encode_sentence(sentence, word2idx, max_len)\n",
        "            src_tensor = torch.tensor(token_ids, device=device).unsqueeze(0)  # (1, seq_len)\n",
        "\n",
        "            # inference\n",
        "            hidden = model.encoder.init_hidden(1)\n",
        "            encoder_outputs, hidden = model.encoder(src_tensor.T, hidden)\n",
        "            input_token = torch.tensor([sos_idx], device=device)\n",
        "            predicted_ids = []\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                context = model.compute_attention(hidden, encoder_outputs)\n",
        "                output, hidden = model.decoder(input_token, hidden, context)\n",
        "                top_id = output.argmax(1).item()\n",
        "                if top_id == eos_idx:\n",
        "                    break\n",
        "                predicted_ids.append(top_id)\n",
        "                input_token = torch.tensor([top_id], device=device)\n",
        "\n",
        "            # decode tokens to sentence\n",
        "            words = [idx2word[idx] for idx in predicted_ids if idx not in (sos_idx, eos_idx, pad_idx)]\n",
        "            predictions.append(\" \".join(words))\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "72rkUHt0Bto9"
      },
      "id": "72rkUHt0Bto9",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"test_no_target.csv\")\n",
        "test_sentences = test_df['input_sentence'].tolist()\n"
      ],
      "metadata": {
        "id": "GEt-WHkiAhGd"
      },
      "id": "GEt-WHkiAhGd",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = infer_sentences(test_sentences, attention_model, word2idx, idx2word, device)\n"
      ],
      "metadata": {
        "id": "QNLgttT1AhJl"
      },
      "id": "QNLgttT1AhJl",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred = pd.DataFrame({'prediction': test_predictions})\n",
        "df_pred.to_csv(\"seq2seq_with_attention_predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Dq5aWA2qAhMW"
      },
      "id": "Dq5aWA2qAhMW",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"seq2seq_with_attention_predictions.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mv8EAFGPA-gc",
        "outputId": "170c9305-e5c6-4f2e-c2a5-9a7ff77f97e4"
      },
      "id": "mv8EAFGPA-gc",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_19219032-106f-4973-843f-91aad6e7f975\", \"seq2seq_with_attention_predictions.csv\", 140910)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "by0tgQj3A-js"
      },
      "id": "by0tgQj3A-js",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WwdJJDrjA-mc"
      },
      "id": "WwdJJDrjA-mc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "dd4a4eb5",
      "metadata": {
        "id": "dd4a4eb5"
      },
      "outputs": [],
      "source": [
        "# without the attention\n",
        "def infer_seq2seq_basic(shuffled_sentences, model, word2idx, idx2word, device, max_len=16):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    sos_idx = word2idx['<SOS>']\n",
        "    eos_idx = word2idx['<EOS>']\n",
        "    pad_idx = word2idx['<PAD>']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentence in shuffled_sentences:\n",
        "            # encode sentence\n",
        "            token_ids = encode_sentence(sentence, word2idx, max_len)\n",
        "            src_tensor = torch.tensor(token_ids, device=device).unsqueeze(0)  # (1, seq_len)\n",
        "\n",
        "            # encode\n",
        "            hidden = model.encoder.init_hidden(1)\n",
        "            _, hidden = model.encoder(src_tensor.T, hidden)\n",
        "\n",
        "            input_token = torch.tensor([sos_idx], device=device)\n",
        "            predicted_ids = []\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                output, hidden = model.decoder(input_token, hidden)\n",
        "                top_id = output.argmax(1).item()\n",
        "                if top_id == eos_idx:\n",
        "                    break\n",
        "                predicted_ids.append(top_id)\n",
        "                input_token = torch.tensor([top_id], device=device)\n",
        "\n",
        "            # convert to words\n",
        "            words = [idx2word[idx] for idx in predicted_ids if idx not in [sos_idx, eos_idx, pad_idx]]\n",
        "            predicted_sentence = \" \".join(words)\n",
        "            predictions.append(predicted_sentence)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "461c225d",
      "metadata": {
        "id": "461c225d"
      },
      "outputs": [],
      "source": [
        "basic_preds = infer_seq2seq_basic(test_sentences, model, word2idx, idx2word, device)\n",
        "\n",
        "# save to file\n",
        "pd.DataFrame({'prediction': basic_preds}).to_csv(\"seq2seq_predictions.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "59f78a00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "59f78a00",
        "outputId": "ecb0c548-453c-4a68-bb9b-36a84b418b0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bae3a5fc-c640-4d50-80d9-b83bbcfbef0c\", \"seq2seq_predictions.csv\", 174801)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"seq2seq_predictions.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7o3Mz8zr3foD"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}